{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3094c833",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2305b3f9",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aa233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install h5py\n",
    "!pip install astropy\n",
    "!pip install scipy\n",
    "!pip install pycwt\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "148945e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pycwt as wavelet\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy.units as unit\n",
    "import astropy.constants as cons\n",
    "from astropy.time import Time \n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "filePath = \"../data/PlanetaryEphemerides.h5\"\n",
    "\n",
    "solar_mass = 1.989e30 # kg\n",
    "\n",
    "__ANGLE_LIMIT_LOWER__ = 86\n",
    "__ANGLE_LIMIT_HIGHER__ = 94\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='erfa')\n",
    "\n",
    "class FancyColors:\n",
    "    TEST = \"\\033[1;4;93;41m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "    IMPORTANT = \"\\033[1;4;96;41m\"\n",
    "\n",
    "    RED = \"\\033[31m\"\n",
    "    GREEN = \"\\033[32m\"\n",
    "    YELLOW = \"\\033[33m\"\n",
    "    BLUE = \"\\033[34m\"\n",
    "    MAGENTA = \"\\033[35m\"\n",
    "    CYAN = \"\\033[36m\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c464c5fb",
   "metadata": {},
   "source": [
    "## Define statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e801c80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def circ_lin_r(angle, y):\n",
    "    angle = np.asarray(angle)\n",
    "    y = np.asarray(y)\n",
    "    x = (y - y.mean()) / y.std()\n",
    "    c = np.cos(angle)\n",
    "    s = np.sin(angle)\n",
    "    r_xc = pearsonr(x, c)[0]\n",
    "    r_xs = pearsonr(x, s)[0]\n",
    "    r_cs = pearsonr(c, s)[0]\n",
    "    return np.sqrt((r_xc**2 + r_xs**2 - 2*r_xc*r_xs*r_cs) / (1 - r_cs**2 + 1e-15)), x, c, s, r_cs\n",
    "\n",
    "def sweep_lag_circ_lin(\n",
    "    t_angle, angle, t_y, y, \n",
    "    lags,                      # iterable of lags in same units as t_* (e.g., days)\n",
    "    L=30,                      # block length for perms (≈ autocorr time)\n",
    "    n_perm=2000\n",
    "):\n",
    "    # Make DataFrames to inner-join on time after lagging\n",
    "    da = pd.DataFrame({\"t\": t_angle, \"angle\": angle})\n",
    "    dy0 = pd.DataFrame({\"t\": t_y, \"y\": y}).sort_values(\"t\").reset_index(drop=True)\n",
    "\n",
    "    results = []\n",
    "    for lag in lags:\n",
    "        # shift y in time by 'lag': y(t+lag)\n",
    "        dy = dy0.copy()\n",
    "        dy[\"t\"] = dy[\"t\"] + lag\n",
    "        merged = pd.merge(da, dy, on=\"t\", how=\"inner\")\n",
    "        if len(merged) < 10:\n",
    "            results.append({\"lag\": lag, \"r\": np.nan})\n",
    "            continue\n",
    "        r,_,_,_,_ = circ_lin_r(merged[\"angle\"].values, merged[\"y\"].values)\n",
    "        results.append({\"lag\": lag, \"r\": r})\n",
    "    res = pd.DataFrame(results)\n",
    "    if res[\"r\"].notna().sum() == 0:\n",
    "        return {\"table\": res, \"best_lag\": np.nan, \"best_r\": np.nan, \"p_fwe\": np.nan}\n",
    "\n",
    "    # Observed max statistic across lags\n",
    "    best_idx = res[\"r\"].idxmax()\n",
    "    obs_max_r = res.loc[best_idx, \"r\"]\n",
    "\n",
    "    # --- max-T block-permutation (family-wise error across lags) ---\n",
    "    # Build block indices on the original (unshifted) y to preserve autocorr\n",
    "    W = len(dy0)\n",
    "    idx = np.arange(W)\n",
    "    blocks = [idx[i:i+L] for i in range(0, W, L)]\n",
    "\n",
    "    max_r_perm = []\n",
    "    y_arr = dy0[\"y\"].values\n",
    "    for _ in range(n_perm):\n",
    "        order = np.random.permutation(len(blocks))\n",
    "        pidx = np.concatenate([blocks[i] for i in order])\n",
    "        dy_p = dy0.copy()\n",
    "        dy_p[\"y\"] = y_arr[pidx]\n",
    "\n",
    "        max_r_this_perm = -np.inf\n",
    "        for lag in lags:\n",
    "            dyp = dy_p.copy()\n",
    "            dyp[\"t\"] = dyp[\"t\"] + lag\n",
    "            merged = pd.merge(da, dyp, on=\"t\", how=\"inner\")\n",
    "            if len(merged) < 10:\n",
    "                continue\n",
    "            r_p,_,_,_,_ = circ_lin_r(merged[\"angle\"].values, merged[\"y\"].values)\n",
    "            if r_p > max_r_this_perm:\n",
    "                max_r_this_perm = r_p\n",
    "        if np.isfinite(max_r_this_perm):\n",
    "            max_r_perm.append(max_r_this_perm)\n",
    "\n",
    "    # Family-wise p-value: fraction of permuted max-r >= observed max-r\n",
    "    if len(max_r_perm) == 0:\n",
    "        p_fwe = np.nan\n",
    "    else:\n",
    "        max_r_perm = np.asarray(max_r_perm)\n",
    "        p_fwe = (np.sum(max_r_perm >= obs_max_r) + 1) / (len(max_r_perm) + 1)\n",
    "\n",
    "    return {\n",
    "        \"table\": res.sort_values(\"lag\").reset_index(drop=True),\n",
    "        \"best_lag\": res.loc[best_idx, \"lag\"],\n",
    "        \"best_r\": obs_max_r,\n",
    "        \"p_fwe\": p_fwe,  # corrected for multiple lags\n",
    "    }\n",
    "\n",
    "def circ_lin_corr(angle, y):\n",
    "\n",
    "    r, x, c, s, r_cs = circ_lin_r(angle, y)\n",
    "\n",
    "    # --- Block permutation p-value (choose block length L ~ dominant autocorr time) ---\n",
    "    def block_perm_corr(x, c, s, r_cs, L=30, n_perm=5000):\n",
    "        W = len(x)\n",
    "        idx = np.arange(W)\n",
    "        blocks = [idx[i:i+L] for i in range(0, W, L)]\n",
    "        obs = r\n",
    "        cnt = 0\n",
    "        for _ in range(n_perm):\n",
    "            # shuffle blocks\n",
    "            order = np.random.permutation(len(blocks))\n",
    "            pidx = np.concatenate([blocks[i] for i in order])\n",
    "            xp = x[pidx]\n",
    "            r_xc_p = pearsonr(xp, c)[0]\n",
    "            r_xs_p = pearsonr(xp, s)[0]\n",
    "            r_p = np.sqrt((r_xc_p**2 + r_xs_p**2 - 2*r_xc_p*r_xs_p*r_cs) / (1 - r_cs**2))\n",
    "            cnt += (r_p >= obs)\n",
    "        return (cnt+1)/(n_perm+1)\n",
    "\n",
    "    p_vals = []\n",
    "    for L in [10,20,30,40,60,80,120]:\n",
    "        p_vals.append(block_perm_corr(x, c, s, r_cs, L=30, n_perm=2000))\n",
    "    return {\"pearsonr\":r, \"pscore\":np.mean(p_vals)}\n",
    "\n",
    "def wavelet_cross_analysis(x, y, dt, mother=wavelet.Morlet(6), dj=1/12, s0=None, J=None):\n",
    "    \"\"\"\n",
    "    Compute individual CWTs, cross-wavelet transform (XWT),\n",
    "    coherence (WCT), phase, and time delay using native pycwt.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    # --- Define scales ---\n",
    "    if s0 is None:\n",
    "        s0 = 2 * dt\n",
    "    if J is None:\n",
    "        J = int(np.log2(len(x) * dt / s0) / dj)\n",
    "\n",
    "    # --- Individual continuous wavelet transforms ---\n",
    "    Wx, scales, freqs, coi_x, _, _ = wavelet.cwt(x, dt, dj, s0, J, mother)\n",
    "    Wy, _, _, coi_y, _, _ = wavelet.cwt(y, dt, dj, s0, J, mother)\n",
    "    period = 1 / freqs\n",
    "    coi = np.minimum(coi_x, coi_y)  # conservative COI\n",
    "\n",
    "    # --- Cross-wavelet transform (XWT) ---\n",
    "    Wxy = Wx * np.conj(Wy)\n",
    "    xwt_power = np.abs(Wxy)\n",
    "\n",
    "    # --- Wavelet coherence (WCT) ---\n",
    "    WCT, aWCT, coi_wct, freq_wct, sig = wavelet.wct(x, y, dt, dj=dj, s0=s0, J=J, mother=mother, sig = False) # Disable monte-carlo simulation with sig=False\n",
    "\n",
    "    # --- Phase and delay ---\n",
    "    phase = np.angle(aWCT)\n",
    "    delay = (phase / (2 * np.pi)) * (1 / freq_wct)[:, None]\n",
    "\n",
    "    return {\n",
    "        \"period\": period,\n",
    "        \"freq\": freqs,\n",
    "        \"Wx\": Wx,\n",
    "        \"Wy\": Wy,\n",
    "        \"Wxy\": Wxy,\n",
    "        \"xwt\": xwt_power,\n",
    "        \"wtc\": WCT,\n",
    "        \"aWCT\": aWCT,\n",
    "        \"phase\": phase,\n",
    "        \"delay\": delay,\n",
    "        \"sig\": sig,\n",
    "        \"coi\": coi,\n",
    "        \"coi_wct\": coi_wct,\n",
    "    }\n",
    "\n",
    "def variance_ratio_scan(array, interval1_size: int = 10, interval2_size: int = 5, inverse: bool = False):\n",
    "    \"\"\"\n",
    "    Computes running variance ratios across an array with circular wrapping.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    array : numpy array\n",
    "        The input data array\n",
    "    interval_size : int\n",
    "        The size of each interval for variance computation\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ratios : numpy array\n",
    "        Array of variance ratios (variance of second interval / variance of first interval)\n",
    "    \"\"\"\n",
    "    n = len(array)\n",
    "    ratios = np.zeros(n)\n",
    "\n",
    "    for i in range(n):\n",
    "        # Define first interval starting at index i\n",
    "        interval1_indices = np.arange(i, i + interval1_size) % n\n",
    "        interval1 = array[interval1_indices]\n",
    "\n",
    "        # Define second interval starting at index i + interval1_size\n",
    "        interval2_indices = np.arange(i + interval1_size, i + interval1_size + interval2_size) % n\n",
    "        interval2 = array[interval2_indices]\n",
    "\n",
    "        # Compute variances\n",
    "        var1 = np.var(interval1)\n",
    "        var2 = np.var(interval2)\n",
    "\n",
    "        # Compute ratio (handle division by zero)\n",
    "        if not inverse:\n",
    "            if var1 != 0:\n",
    "                ratios[i] = var2 / var1\n",
    "            else:\n",
    "                ratios[i] = np.nan  # or np.inf, depending on your preference\n",
    "        else:\n",
    "            if var2 != 0 :\n",
    "                ratios[i] = var1 / var2\n",
    "            else:\n",
    "                ratios[i] = np.nan\n",
    "\n",
    "    return ratios\n",
    "\n",
    "def angle_window_variance_ratio(time_array, angle_array, data_array, \n",
    "                                  angle_min, angle_max, pre_steps, inverse: bool = False):\n",
    "    \"\"\"\n",
    "    Computes variance ratios for data during angle windows vs pre-window periods.\n",
    "    \"\"\"\n",
    "    # Find indices where angle is within the specified range\n",
    "    in_window = (angle_array >= angle_min) & (angle_array <= angle_max)\n",
    "\n",
    "    # Find contiguous windows\n",
    "    windows = []\n",
    "    start_idx = None\n",
    "\n",
    "    for i in range(len(angle_array)):\n",
    "        if in_window[i] and start_idx is None:\n",
    "            # Start of a new window\n",
    "            start_idx = i\n",
    "        elif not in_window[i] and start_idx is not None:\n",
    "            # End of a window\n",
    "            windows.append((start_idx, i - 1))\n",
    "            start_idx = None\n",
    "\n",
    "    # Handle case where window extends to end of array\n",
    "    if start_idx is not None:\n",
    "        windows.append((start_idx, len(angle_array) - 1))\n",
    "\n",
    "    # Compute variance ratios for each window\n",
    "    window_info = []\n",
    "    pre_data = None\n",
    "\n",
    "    for start, end in windows:\n",
    "        # Variance during the window\n",
    "        window_data = data_array[start:end+1]\n",
    "        var_during = np.var(window_data)\n",
    "\n",
    "        # Variance before the window (pre_steps timesteps before start)\n",
    "        pre_start = start - pre_steps\n",
    "        pre_end = start - 1\n",
    "\n",
    "        if pre_start >= 0:\n",
    "            pre_data = data_array[pre_start:pre_end+1]\n",
    "            var_before = np.var(pre_data)\n",
    "\n",
    "            # Compute ratio\n",
    "            if not inverse:\n",
    "                if var_before != 0:\n",
    "                    ratio = var_during / var_before\n",
    "                elif (var_during == 0) & (var_before == 0):\n",
    "                    ratio = 0\n",
    "                else:\n",
    "                    ratio = np.nan\n",
    "            else:\n",
    "                if var_during != 0:\n",
    "                    ratio = var_before / var_during\n",
    "                elif (var_during == 0 ) & (var_before == 0):\n",
    "                    ratio = 0\n",
    "                else:\n",
    "                    ratio = np.nan\n",
    "        else:\n",
    "            # Not enough data before window\n",
    "            ratio = np.nan\n",
    "\n",
    "        window_info.append({\n",
    "            'start_idx': start,\n",
    "            'end_idx': end,\n",
    "            'ratio': ratio,\n",
    "            'start_time': time_array[start],\n",
    "            'end_time': time_array[end],\n",
    "            'var_during': var_during,\n",
    "            'var_before': var_before if pre_start >= 0 else np.nan,\n",
    "            'window_data': window_data,\n",
    "            'pre_data': pre_data if len(windows)>0 else None\n",
    "        })\n",
    "\n",
    "    return window_info\n",
    "\n",
    "def angle_window_variance_ratio_post(time_array, angle_array, data_array, \n",
    "                                    angle_min, angle_max, post_steps, inverse: bool = False):\n",
    "    \"\"\"\n",
    "    Computes variance ratios for data during angle windows vs post-window periods.\n",
    "    \"\"\"\n",
    "    # Find indices where angle is within the specified range\n",
    "    in_window = (angle_array >= angle_min) & (angle_array <= angle_max)\n",
    "\n",
    "    # Find contiguous windows\n",
    "    windows = []\n",
    "    start_idx = None\n",
    "\n",
    "    for i in range(len(angle_array)):\n",
    "        if in_window[i] and start_idx is None:\n",
    "            # Start of a new window\n",
    "            start_idx = i\n",
    "        elif not in_window[i] and start_idx is not None:\n",
    "            # End of a window\n",
    "            windows.append((start_idx, i - 1))\n",
    "            start_idx = None\n",
    "\n",
    "    # Handle case where window extends to end of array\n",
    "    if start_idx is not None:\n",
    "        windows.append((start_idx, len(angle_array) - 1))\n",
    "\n",
    "    # Compute variance ratios for each window\n",
    "    window_info = []\n",
    "    post_data = None\n",
    "\n",
    "    for start, end in windows:\n",
    "        # Variance during the window\n",
    "        window_data = data_array[start:end+1]\n",
    "        var_during = np.var(window_data)\n",
    "\n",
    "        # Variance after the window (post_steps timesteps after end)\n",
    "        post_start = end + 1\n",
    "        post_end = end + post_steps\n",
    "\n",
    "        if post_end < len(data_array):\n",
    "            post_data = data_array[post_start:post_end+1]\n",
    "            var_after = np.var(post_data)\n",
    "\n",
    "            # Compute ratio\n",
    "            if not inverse:\n",
    "                if var_after != 0:\n",
    "                    ratio = var_during / var_after\n",
    "                elif (var_during == 0) & (var_after == 0):\n",
    "                    ratio = 0\n",
    "                else:\n",
    "                    ratio = np.nan\n",
    "            else:\n",
    "                if var_during != 0:\n",
    "                    ratio = var_after / var_during\n",
    "                elif (var_during == 0) & (var_after == 0):\n",
    "                    ratio = 0\n",
    "                else:\n",
    "                    ratio = np.nan\n",
    "        else:\n",
    "            # Not enough data after window\n",
    "            ratio = np.nan\n",
    "            post_data = np.array([])\n",
    "            var_after = np.nan\n",
    "\n",
    "        window_info.append({\n",
    "            'start_idx': start,\n",
    "            'end_idx': end,\n",
    "            'ratio': ratio,\n",
    "            'start_time': time_array[start],\n",
    "            'end_time': time_array[end],\n",
    "            'var_during': var_during,\n",
    "            'var_after': var_after,\n",
    "            'window_data': window_data,\n",
    "            'post_data': post_data\n",
    "        })\n",
    "\n",
    "    return window_info\n",
    "\n",
    "\n",
    "def angle_window_post_event_difference(time_array, angle_array, data_array,\n",
    "                                       angle_min, angle_max, \n",
    "                                       weeks_after, \n",
    "                                       n_points_at_end=1,\n",
    "                                       n_points_after=1):\n",
    "    \"\"\"\n",
    "    Computes the difference between data mean at event end vs X weeks after event end.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    time_array : numpy array\n",
    "        Time series in Julian dates\n",
    "    angle_array : numpy array\n",
    "        Angle values corresponding to time_array\n",
    "    data_array : numpy array\n",
    "        Data values corresponding to time_array\n",
    "    angle_min : float\n",
    "        Minimum angle defining the event window (default 86 degrees)\n",
    "    angle_max : float\n",
    "        Maximum angle defining the event window (default 94 degrees)\n",
    "    weeks_after : int\n",
    "        Number of weeks after event end to compare\n",
    "    n_points_at_end : int\n",
    "        Number of points to average around event end (1 = single point)\n",
    "    n_points_after : int\n",
    "        Number of points to average around X-weeks-after point\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    event_info : list of dicts\n",
    "        Each dict contains:\n",
    "        - end_idx: Index where event ends\n",
    "        - end_time: Time when event ends\n",
    "        - mean_at_end: Mean of data at event end\n",
    "        - mean_after: Mean of data X weeks after\n",
    "        - difference: mean_after - mean_at_end\n",
    "        - std_at_end: Standard deviation at event end\n",
    "        - std_after: Standard deviation X weeks after\n",
    "        - error: Propagated error of the difference\n",
    "        - data_at_end: Data points used for mean at end\n",
    "        - data_after: Data points used for mean after\n",
    "    \"\"\"\n",
    "    # Find indices where angle is within the specified range\n",
    "    in_window = (angle_array >= angle_min) & (angle_array <= angle_max)\n",
    "    \n",
    "    # Find contiguous windows\n",
    "    windows = []\n",
    "    start_idx = None\n",
    "    \n",
    "    for i in range(len(angle_array)):\n",
    "        if in_window[i] and start_idx is None:\n",
    "            start_idx = i\n",
    "        elif not in_window[i] and start_idx is not None:\n",
    "            windows.append((start_idx, i - 1))\n",
    "            start_idx = None\n",
    "    \n",
    "    if start_idx is not None:\n",
    "        windows.append((start_idx, len(angle_array) - 1))\n",
    "    \n",
    "    event_info = []\n",
    "    \n",
    "    for start, end in windows:\n",
    "        end_idx = end\n",
    "        \n",
    "        # Compute indices for averaging at event end\n",
    "        if n_points_at_end == 1:\n",
    "            idx_at_end = [end_idx]\n",
    "        else:\n",
    "            half_window = n_points_at_end // 2\n",
    "            idx_at_end = list(range(end_idx - half_window, end_idx + half_window + 1))\n",
    "            idx_at_end = [i for i in idx_at_end if 0 <= i < len(data_array)]\n",
    "        \n",
    "        # Compute mean and std at event end\n",
    "        data_at_end = data_array[idx_at_end]\n",
    "        mean_at_end = np.nanmean(data_at_end)\n",
    "        std_at_end = np.nanstd(data_at_end)\n",
    "        \n",
    "        # Find index X weeks after event end\n",
    "        # Assuming daily data: weeks_after * 7 days\n",
    "        days_after = weeks_after * 7\n",
    "        after_idx = end_idx + days_after\n",
    "        \n",
    "        if after_idx >= len(data_array):\n",
    "            # Not enough data after event\n",
    "            event_info.append({\n",
    "                'end_idx': end_idx,\n",
    "                'after_idx': after_idx,\n",
    "                'end_time': time_array[end_idx],\n",
    "                'mean_at_end': mean_at_end,\n",
    "                'mean_after': np.nan,\n",
    "                'difference': np.nan,\n",
    "                'std_at_end': std_at_end,\n",
    "                'std_after': np.nan,\n",
    "                'error': np.nan,\n",
    "                'data_at_end': data_at_end,\n",
    "                'data_after': np.array([])\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Compute indices for averaging X weeks after\n",
    "        if n_points_after == 1:\n",
    "            idx_after = [after_idx]\n",
    "        else:\n",
    "            half_window = n_points_after // 2\n",
    "            idx_after = list(range(after_idx - half_window, after_idx + half_window + 1))\n",
    "            idx_after = [i for i in idx_after if 0 <= i < len(data_array)]\n",
    "        \n",
    "        # Compute mean and std X weeks after\n",
    "        data_after = data_array[idx_after]\n",
    "        mean_after = np.nanmean(data_after)\n",
    "        std_after = np.nanstd(data_after)\n",
    "        \n",
    "        # Compute difference\n",
    "        difference = mean_after - mean_at_end\n",
    "        \n",
    "        # Propagate error (assuming independent measurements)\n",
    "        n_at_end = len(data_at_end)\n",
    "        n_after = len(data_after)\n",
    "        error_at_end = std_at_end / np.sqrt(n_at_end) if n_at_end > 0 else np.nan\n",
    "        error_after = std_after / np.sqrt(n_after) if n_after > 0 else np.nan\n",
    "        error = np.sqrt(error_at_end**2 + error_after**2)\n",
    "        \n",
    "        event_info.append({\n",
    "            'end_idx': end_idx,\n",
    "            'after_idx': after_idx,\n",
    "            'end_time': time_array[end_idx],\n",
    "            'mean_at_end': mean_at_end,\n",
    "            'mean_after': mean_after,\n",
    "            'difference': difference,\n",
    "            'std_at_end': std_at_end,\n",
    "            'std_after': std_after,\n",
    "            'error': error,\n",
    "            'data_at_end': data_at_end,\n",
    "            'data_after': data_after\n",
    "        })\n",
    "    \n",
    "    return event_info\n",
    "\n",
    "\n",
    "def angle_window_post_event_gradient(time_array, angle_array, data_array,\n",
    "                                     angle_min, angle_max,\n",
    "                                     weeks_after,\n",
    "                                     error_window=5):\n",
    "    \"\"\"\n",
    "    Computes the average gradient from event end to X weeks after event end.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    time_array : numpy array\n",
    "        Time series in Julian dates\n",
    "    angle_array : numpy array\n",
    "        Angle values corresponding to time_array\n",
    "    data_array : numpy array\n",
    "        Data values corresponding to time_array\n",
    "    angle_min : float\n",
    "        Minimum angle defining the event window\n",
    "    angle_max : float\n",
    "        Maximum angle defining the event window\n",
    "    weeks_after : int\n",
    "        Number of weeks after event end to compute gradient\n",
    "    error_window : int, optional\n",
    "        Number of points around each endpoint to use for error estimation (default=5)\n",
    "        Set to 0 to disable error computation\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    event_info : list of dicts\n",
    "        Each dict contains:\n",
    "        - end_idx: Index where event ends\n",
    "        - end_time: Time when event ends\n",
    "        - after_idx: Index X weeks after\n",
    "        - after_time: Time X weeks after\n",
    "        - gradient: Average gradient (change in data per day)\n",
    "        - gradient_per_week: Gradient in units per week\n",
    "        - gradient_error: Error in gradient (per day)\n",
    "        - gradient_error_per_week: Error in gradient (per week)\n",
    "        - data_at_end: Data value at event end\n",
    "        - data_after: Data value X weeks after\n",
    "        - std_at_end: Standard deviation at event end\n",
    "        - std_after: Standard deviation X weeks after\n",
    "        - delta_data: Change in data\n",
    "        - delta_time: Time interval in days\n",
    "    \"\"\"\n",
    "    # Find indices where angle is within the specified range\n",
    "    in_window = (angle_array >= angle_min) & (angle_array <= angle_max)\n",
    "\n",
    "    # Find contiguous windows\n",
    "    windows = []\n",
    "    start_idx = None\n",
    "\n",
    "    for i in range(len(angle_array)):\n",
    "        if in_window[i] and start_idx is None:\n",
    "            start_idx = i\n",
    "        elif not in_window[i] and start_idx is not None:\n",
    "            windows.append((start_idx, i - 1))\n",
    "            start_idx = None\n",
    "\n",
    "    if start_idx is not None:\n",
    "        windows.append((start_idx, len(angle_array) - 1))\n",
    "\n",
    "    event_info = []\n",
    "\n",
    "    for start, end in windows:\n",
    "        end_idx = end\n",
    "\n",
    "        # Find index X weeks after event end\n",
    "        days_after = weeks_after * 7\n",
    "        after_idx = end_idx + days_after\n",
    "\n",
    "        if after_idx >= len(data_array):\n",
    "            # Not enough data after event\n",
    "            event_info.append({\n",
    "                'end_idx': end_idx,\n",
    "                'end_time': time_array[end_idx],\n",
    "                'after_idx': np.nan,\n",
    "                'after_time': np.nan,\n",
    "                'gradient': np.nan,\n",
    "                'gradient_per_week': np.nan,\n",
    "                'gradient_error': np.nan,\n",
    "                'gradient_error_per_week': np.nan,\n",
    "                'data_at_end': data_array[end_idx],\n",
    "                'data_after': np.nan,\n",
    "                'std_at_end': np.nan,\n",
    "                'std_after': np.nan,\n",
    "                'delta_data': np.nan,\n",
    "                'delta_time': np.nan\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Get data and time values\n",
    "        data_at_end = data_array[end_idx]\n",
    "        data_after = data_array[after_idx]\n",
    "        time_at_end = time_array[end_idx]\n",
    "        time_after = time_array[after_idx]\n",
    "\n",
    "        # Compute local standard deviations for error estimation\n",
    "        if error_window > 0:\n",
    "            # Get window around end point\n",
    "            half_window = error_window // 2\n",
    "            idx_at_end = list(range(end_idx - half_window, end_idx + half_window + 1))\n",
    "            idx_at_end = [i for i in idx_at_end if 0 <= i < len(data_array)]\n",
    "\n",
    "            # Get window around after point\n",
    "            idx_after = list(range(after_idx - half_window, after_idx + half_window + 1))\n",
    "            idx_after = [i for i in idx_after if 0 <= i < len(data_array)]\n",
    "\n",
    "            # Compute standard deviations\n",
    "            data_window_end = data_array[idx_at_end]\n",
    "            data_window_after = data_array[idx_after]\n",
    "\n",
    "            std_at_end = np.nanstd(data_window_end)\n",
    "            std_after = np.nanstd(data_window_after)\n",
    "\n",
    "            # Standard errors\n",
    "            n_at_end = len(data_window_end)\n",
    "            n_after = len(data_window_after)\n",
    "            se_at_end = std_at_end / np.sqrt(n_at_end) if n_at_end > 0 else np.nan\n",
    "            se_after = std_after / np.sqrt(n_after) if n_after > 0 else np.nan\n",
    "        else:\n",
    "            std_at_end = np.nan\n",
    "            std_after = np.nan\n",
    "            se_at_end = np.nan\n",
    "            se_after = np.nan\n",
    "\n",
    "        # Compute gradient\n",
    "        delta_data = data_after - data_at_end\n",
    "        delta_time = time_after - time_at_end  # in days (Julian dates)\n",
    "\n",
    "        gradient = delta_data / delta_time if delta_time != 0 else np.nan\n",
    "        gradient_per_week = gradient * 7  # Convert to per week\n",
    "\n",
    "        # Compute gradient error: σ_g = sqrt(σ_end^2 + σ_after^2) / Δt\n",
    "        if error_window > 0 and delta_time != 0:\n",
    "            gradient_error = np.sqrt(se_at_end**2 + se_after**2) / delta_time\n",
    "            gradient_error_per_week = gradient_error * 7\n",
    "        else:\n",
    "            gradient_error = np.nan\n",
    "            gradient_error_per_week = np.nan\n",
    "\n",
    "        event_info.append({\n",
    "            'end_idx': end_idx,\n",
    "            'end_time': time_at_end,\n",
    "            'after_idx': after_idx,\n",
    "            'after_time': time_after,\n",
    "            'gradient': gradient,\n",
    "            'gradient_per_week': gradient_per_week,\n",
    "            'gradient_error': gradient_error,\n",
    "            'gradient_error_per_week': gradient_error_per_week,\n",
    "            'data_at_end': data_at_end,\n",
    "            'data_after': data_after,\n",
    "            'std_at_end': std_at_end,\n",
    "            'std_after': std_after,\n",
    "            'delta_data': delta_data,\n",
    "            'delta_time': delta_time\n",
    "        })\n",
    "\n",
    "    return event_info\n",
    "\n",
    "\n",
    "\n",
    "def random_span_difference_null_distribution(data_array, \n",
    "                                                    span_weeks=4, \n",
    "                                                    n_local_points=1, \n",
    "                                                    n_trials=1000,\n",
    "                                                    random_seed=None):\n",
    "    \"\"\"\n",
    "    Generates a null distribution by computing differences over random time spans.\n",
    "    \n",
    "    This function randomly selects time periods of a specified length and computes\n",
    "    the difference between locally-averaged values at the start and end of each span.\n",
    "    This creates a null distribution for comparison with event-based differences.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_array : numpy array\n",
    "        The data time series\n",
    "    span_weeks : int\n",
    "        Length of each random span in weeks (default=4)\n",
    "    n_local_points : int\n",
    "        Number of points to average at start and end of span (default=1)\n",
    "        If 1, uses single point; if >1, uses centered window\n",
    "    n_trials : int\n",
    "        Number of random trials to perform (default=1000)\n",
    "    random_seed : int, optional\n",
    "        Seed for random number generator for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    result : dict\n",
    "        Dictionary containing:\n",
    "        - differences: array of all computed differences\n",
    "        - percent_positive: percentage of differences > 0\n",
    "        - percent_negative: percentage of differences < 0\n",
    "        - percent_zero: percentage of differences == 0\n",
    "        - mean_difference: mean of all differences\n",
    "        - std_difference: standard deviation of differences\n",
    "        - median_difference: median of differences\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    data_array = np.asarray(data_array)\n",
    "    n_data = len(data_array)\n",
    "    span_days = span_weeks * 7\n",
    "\n",
    "    # Need enough data for span plus local averaging windows\n",
    "    half_window = n_local_points // 2\n",
    "    min_required = span_days + 2 * half_window\n",
    "\n",
    "    if n_data < min_required:\n",
    "        raise ValueError(f\"Data array too short. Need at least {min_required} points.\")\n",
    "\n",
    "    differences = []\n",
    "\n",
    "    for _ in range(n_trials):\n",
    "        # Randomly select start of span\n",
    "        # Ensure we have room for the span and local averaging windows\n",
    "        max_start = n_data - span_days - half_window\n",
    "        if max_start < half_window:\n",
    "            raise ValueError(\"Not enough data for the specified span and averaging window.\")\n",
    "\n",
    "        start_idx = np.random.randint(half_window, max_start)\n",
    "        end_idx = start_idx + span_days\n",
    "\n",
    "        # Compute local average at start\n",
    "        if n_local_points == 1:\n",
    "            idx_at_start = [start_idx]\n",
    "        else:\n",
    "            idx_at_start = list(range(start_idx - half_window, start_idx + half_window + 1))\n",
    "            idx_at_start = [i for i in idx_at_start if 0 <= i < n_data]\n",
    "\n",
    "        data_at_start = data_array[idx_at_start]\n",
    "        mean_at_start = np.nanmean(data_at_start)\n",
    "\n",
    "        # Compute local average at end\n",
    "        if n_local_points == 1:\n",
    "            idx_at_end = [end_idx]\n",
    "        else:\n",
    "            idx_at_end = list(range(end_idx - half_window, end_idx + half_window + 1))\n",
    "            idx_at_end = [i for i in idx_at_end if 0 <= i < n_data]\n",
    "\n",
    "        data_at_end = data_array[idx_at_end]\n",
    "        mean_at_end = np.nanmean(data_at_end)\n",
    "\n",
    "        # Compute difference (end - start)\n",
    "        difference = mean_at_end - mean_at_start\n",
    "        differences.append(difference)\n",
    "\n",
    "    differences = np.array(differences)\n",
    "\n",
    "    # Compute statistics\n",
    "    n_positive = np.sum(differences > 0)\n",
    "    n_negative = np.sum(differences < 0)\n",
    "    n_zero = np.sum(differences == 0)\n",
    "\n",
    "    percent_positive = 100 * n_positive / n_trials\n",
    "    percent_negative = 100 * n_negative / n_trials\n",
    "    percent_zero = 100 * n_zero / n_trials\n",
    "\n",
    "    return {\n",
    "        'differences': differences,\n",
    "        'percent_positive': percent_positive,\n",
    "        'percent_negative': percent_negative,\n",
    "        'percent_zero': percent_zero,\n",
    "        'mean_difference': np.mean(differences),\n",
    "        'std_difference': np.std(differences),\n",
    "        'median_difference': np.median(differences),\n",
    "        'n_trials': n_trials\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "poisson_p = lambda exp, obs: 2 * min(poisson.cdf(exp, obs), poisson.sf(exp-1, obs))\n",
    "binom_p = lambda exp, obs: scipy.stats.binomtest(exp,obs, p=0.5).pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28hprwruur1i",
   "metadata": {},
   "source": [
    "## Test Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ewurk4fs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test angle_window_variance_ratio\n",
    "print(\"Testing angle_window_variance_ratio function:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create synthetic test data\n",
    "n_points = 100\n",
    "test_time = np.arange(n_points)\n",
    "test_angles = np.linspace(0, 180, n_points)  # angles from 0 to 180\n",
    "\n",
    "# Create data with known variance pattern:\n",
    "# Low variance (0-1) before index 40\n",
    "# High variance (random) between indices 40-60 (the \"window\")\n",
    "# Low variance (0-1) after index 60\n",
    "np.random.seed(42)\n",
    "test_data = np.ones(n_points) * 0.5  # baseline\n",
    "test_data[:40] += np.random.normal(0, 0.1, 40)  # low variance pre-window\n",
    "test_data[40:60] += np.random.normal(0, 2.0, 20)  # high variance during window\n",
    "test_data[60:] += np.random.normal(0, 0.1, 40)  # low variance post-window\n",
    "\n",
    "# Test the function: look for angles between 70-110 degrees\n",
    "# This should capture roughly the high-variance region\n",
    "result = angle_window_variance_ratio(\n",
    "    test_time, \n",
    "    test_angles, \n",
    "    test_data,\n",
    "    angle_min=70,\n",
    "    angle_max=110,\n",
    "    pre_steps=10,\n",
    "    inverse=False\n",
    ")\n",
    "\n",
    "print(f\"\\nNumber of windows found: {len(result)}\")\n",
    "for i, window in enumerate(result):\n",
    "    print(f\"\\nWindow {i+1}:\")\n",
    "    print(f\"  Start index: {window['start_idx']}, End index: {window['end_idx']}\")\n",
    "    print(f\"  Variance before: {window['var_before']:.4f}\")\n",
    "    print(f\"  Variance during: {window['var_during']:.4f}\")\n",
    "    print(f\"  Ratio (during/before): {window['ratio']:.4f}\")\n",
    "    if window['ratio'] > 1:\n",
    "        print(f\"  ✓ During-window variance is HIGHER (as expected for our test data)\")\n",
    "    else:\n",
    "        print(f\"  ✗ During-window variance is LOWER\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Test complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kninlcm0kf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test angle_window_variance_ratio_post\n",
    "print(\"Testing angle_window_variance_ratio_post function:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create synthetic test data\n",
    "n_points = 100\n",
    "test_time = np.arange(n_points)\n",
    "test_angles = np.linspace(0, 180, n_points)  # angles from 0 to 180\n",
    "\n",
    "# Create data with known variance pattern:\n",
    "# Low variance before index 40\n",
    "# High variance between indices 40-60 (the \"window\")\n",
    "# Low variance after index 60\n",
    "np.random.seed(42)\n",
    "test_data = np.ones(n_points) * 0.5  # baseline\n",
    "test_data[:40] += np.random.normal(0, 0.1, 40)  # low variance pre-window\n",
    "test_data[40:60] += np.random.normal(0, 2.0, 20)  # high variance during window\n",
    "test_data[60:] += np.random.normal(0, 0.1, 40)  # low variance post-window\n",
    "\n",
    "# Test the function: look for angles between 70-110 degrees\n",
    "# This should capture roughly the high-variance region\n",
    "result_post = angle_window_variance_ratio_post(\n",
    "    test_time, \n",
    "    test_angles, \n",
    "    test_data,\n",
    "    angle_min=70,\n",
    "    angle_max=110,\n",
    "    post_steps=10,\n",
    "    inverse=False\n",
    ")\n",
    "\n",
    "print(f\"\\nNumber of windows found: {len(result_post)}\")\n",
    "for i, window in enumerate(result_post):\n",
    "    print(f\"\\nWindow {i+1}:\")\n",
    "    print(f\"  Start index: {window['start_idx']}, End index: {window['end_idx']}\")\n",
    "    print(f\"  Variance during: {window['var_during']:.4f}\")\n",
    "    print(f\"  Variance after: {window['var_after']:.4f}\")\n",
    "    print(f\"  Ratio (during/after): {window['ratio']:.4f}\")\n",
    "    if window['ratio'] > 1:\n",
    "        print(f\"  ✓ During-window variance is HIGHER (as expected for our test data)\")\n",
    "    else:\n",
    "        print(f\"  ✗ During-window variance is LOWER\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Test complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t2o3udv8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge case tests for both functions\n",
    "print(\"Testing edge cases and special scenarios:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: Multiple angle windows\n",
    "print(\"\\n1. Testing with multiple 90-degree angle windows:\")\n",
    "n_points = 200\n",
    "test_time = np.arange(n_points)\n",
    "# Create angles that cross 90 degrees multiple times\n",
    "test_angles = 90 + 10 * np.sin(2 * np.pi * test_time / 50)\n",
    "test_data = np.random.normal(1.0, 0.5, n_points)\n",
    "\n",
    "result_multi = angle_window_variance_ratio(\n",
    "    test_time, test_angles, test_data,\n",
    "    angle_min=88, angle_max=92, pre_steps=5, inverse=False\n",
    ")\n",
    "print(f\"   Found {len(result_multi)} windows\")\n",
    "\n",
    "# Test 2: Zero variance case\n",
    "print(\"\\n2. Testing with constant (zero variance) data:\")\n",
    "test_data_const = np.ones(100)\n",
    "result_zero = angle_window_variance_ratio(\n",
    "    np.arange(100), np.linspace(0, 180, 100), test_data_const,\n",
    "    angle_min=70, angle_max=110, pre_steps=5, inverse=False\n",
    ")\n",
    "if len(result_zero) > 0:\n",
    "    print(f\"   Ratio with zero variance: {result_zero[0]['ratio']}\")\n",
    "    print(f\"   (Should be 0 when both variances are zero)\")\n",
    "\n",
    "# Test 3: Test inverse parameter\n",
    "print(\"\\n3. Testing inverse=True parameter:\")\n",
    "np.random.seed(123)\n",
    "test_data_inv = np.random.normal(0, 1, 100)\n",
    "result_normal = angle_window_variance_ratio(\n",
    "    np.arange(100), np.linspace(0, 180, 100), test_data_inv,\n",
    "    angle_min=70, angle_max=110, pre_steps=10, inverse=False\n",
    ")\n",
    "result_inverse = angle_window_variance_ratio(\n",
    "    np.arange(100), np.linspace(0, 180, 100), test_data_inv,\n",
    "    angle_min=70, angle_max=110, pre_steps=10, inverse=True\n",
    ")\n",
    "if len(result_normal) > 0 and len(result_inverse) > 0:\n",
    "    ratio_normal = result_normal[0]['ratio']\n",
    "    ratio_inverse = result_inverse[0]['ratio']\n",
    "    print(f\"   Normal ratio: {ratio_normal:.4f}\")\n",
    "    print(f\"   Inverse ratio: {ratio_inverse:.4f}\")\n",
    "    print(f\"   Product (should be ~1): {ratio_normal * ratio_inverse:.4f}\")\n",
    "\n",
    "# Test 4: Insufficient data before window\n",
    "print(\"\\n4. Testing edge case: window too close to start (insufficient pre-data):\")\n",
    "result_edge = angle_window_variance_ratio(\n",
    "    np.arange(50), np.linspace(0, 180, 50), np.random.normal(0, 1, 50),\n",
    "    angle_min=5, angle_max=15, pre_steps=20, inverse=False  # Need 20 points before, but window starts early\n",
    ")\n",
    "if len(result_edge) > 0 and np.isnan(result_edge[0]['ratio']):\n",
    "    print(f\"   ✓ Correctly returns NaN when insufficient pre-data\")\n",
    "else:\n",
    "    print(f\"   Result: {result_edge}\")\n",
    "\n",
    "# Test 5: Insufficient data after window (for post function)\n",
    "print(\"\\n5. Testing edge case: window too close to end (insufficient post-data):\")\n",
    "result_edge_post = angle_window_variance_ratio_post(\n",
    "    np.arange(50), np.linspace(0, 180, 50), np.random.normal(0, 1, 50),\n",
    "    angle_min=160, angle_max=175, post_steps=20, inverse=False  # Need 20 points after\n",
    ")\n",
    "if len(result_edge_post) > 0 and np.isnan(result_edge_post[0]['ratio']):\n",
    "    print(f\"   ✓ Correctly returns NaN when insufficient post-data\")\n",
    "else:\n",
    "    print(f\"   Result: {result_edge_post}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Edge case tests complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a06ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test angle_window_post_event_difference\n",
    "print(\"Testing angle_window_post_event_difference function:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create synthetic test data\n",
    "n_points = 200\n",
    "test_time = np.arange(n_points)  # Simulating daily data (Julian dates)\n",
    "test_angles = np.linspace(0, 180, n_points)\n",
    "\n",
    "# Create data with a known pattern:\n",
    "# - Baseline value around 0.5\n",
    "# - At event end (around index 60), value = 0.3\n",
    "# - 4 weeks later (28 days), value increases to 0.8\n",
    "np.random.seed(42)\n",
    "test_data = np.ones(n_points) * 0.5 + np.random.normal(0, 0.05, n_points)\n",
    "\n",
    "# Create a step increase 4 weeks after event end\n",
    "event_end_idx = 60\n",
    "weeks_after = 4\n",
    "days_after = weeks_after * 7  # 28 days\n",
    "\n",
    "# Set values at event end to be low\n",
    "test_data[event_end_idx-2:event_end_idx+2] = 0.3 + np.random.normal(0, 0.02, 4)\n",
    "\n",
    "# Set values 4 weeks after to be high\n",
    "after_idx = event_end_idx + days_after\n",
    "test_data[after_idx-2:after_idx+2] = 0.8 + np.random.normal(0, 0.02, 4)\n",
    "\n",
    "# Test with single point measurements\n",
    "print(\"\\n1. Testing with single point (n_points_at_end=1, n_points_after=1):\")\n",
    "result_single = angle_window_post_event_difference(\n",
    "    test_time, test_angles, test_data,\n",
    "    angle_min=60, angle_max=70,  # Event should end around index 60\n",
    "    weeks_after=4,\n",
    "    n_points_at_end=1,\n",
    "    n_points_after=1\n",
    ")\n",
    "\n",
    "if len(result_single) > 0:\n",
    "    event = result_single[0]\n",
    "    print(f\"   Event end index: {event['end_idx']}\")\n",
    "    print(f\"   Data at end: {event['mean_at_end']:.4f}\")\n",
    "    print(f\"   Data 4 weeks after: {event['mean_after']:.4f}\")\n",
    "    print(f\"   Difference (after - at_end): {event['difference']:.4f}\")\n",
    "    print(f\"   Expected: ~0.5 (since 0.8 - 0.3 = 0.5)\")\n",
    "    print(f\"   Error: {event['error']:.4f}\")\n",
    "\n",
    "# Test with averaging over multiple points\n",
    "print(\"\\n2. Testing with averaging (n_points_at_end=5, n_points_after=5):\")\n",
    "result_avg = angle_window_post_event_difference(\n",
    "    test_time, test_angles, test_data,\n",
    "    angle_min=60, angle_max=70,\n",
    "    weeks_after=4,\n",
    "    n_points_at_end=5,\n",
    "    n_points_after=5\n",
    ")\n",
    "\n",
    "if len(result_avg) > 0:\n",
    "    event = result_avg[0]\n",
    "    print(f\"   Event end index: {event['end_idx']}\")\n",
    "    print(f\"   Mean at end (5 points): {event['mean_at_end']:.4f}\")\n",
    "    print(f\"   Std at end: {event['std_at_end']:.4f}\")\n",
    "    print(f\"   Mean 4 weeks after (5 points): {event['mean_after']:.4f}\")\n",
    "    print(f\"   Std after: {event['std_after']:.4f}\")\n",
    "    print(f\"   Difference: {event['difference']:.4f}\")\n",
    "    print(f\"   Error: {event['error']:.4f}\")\n",
    "    print(f\"   Note: Averaging reduces noise and provides error estimate\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Test complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ppy90q5p26o",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test angle_window_post_event_gradient with error computation\n",
    "print(\"Testing angle_window_post_event_gradient function with error:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create synthetic test data with a known linear trend after event\n",
    "n_points = 200\n",
    "test_time = np.arange(n_points)  # Simulating daily data\n",
    "test_angles = np.linspace(0, 180, n_points)\n",
    "\n",
    "# Create data with linear increase after event end + some noise\n",
    "np.random.seed(123)\n",
    "test_data = np.ones(n_points) * 0.5\n",
    "\n",
    "# Event ends around index 60\n",
    "event_end_idx = 60\n",
    "weeks_after = 4\n",
    "days_after = weeks_after * 7\n",
    "\n",
    "# Create a linear trend: at end = 0.3, after 4 weeks = 0.8\n",
    "# Gradient = (0.8 - 0.3) / 28 days = 0.0179 per day\n",
    "test_data[event_end_idx] = 0.3\n",
    "after_idx = event_end_idx + days_after\n",
    "test_data[after_idx] = 0.8\n",
    "\n",
    "# Fill in linear interpolation between these points with some noise\n",
    "for i in range(event_end_idx + 1, after_idx):\n",
    "    fraction = (i - event_end_idx) / days_after\n",
    "    test_data[i] = 0.3 + fraction * (0.8 - 0.3) + np.random.normal(0, 0.02)\n",
    "\n",
    "# Add noise around the endpoints\n",
    "test_data[event_end_idx-2:event_end_idx+3] += np.random.normal(0, 0.03, 5)\n",
    "test_data[after_idx-2:after_idx+3] += np.random.normal(0, 0.03, 5)\n",
    "\n",
    "print(\"\\n1. Testing gradient with error calculation (error_window=5):\")\n",
    "result_grad = angle_window_post_event_gradient(\n",
    "    test_time, test_angles, test_data,\n",
    "    angle_min=60, angle_max=70,\n",
    "    weeks_after=4,\n",
    "    error_window=5\n",
    ")\n",
    "\n",
    "if len(result_grad) > 0:\n",
    "    event = result_grad[0]\n",
    "    print(f\"   Event end index: {event['end_idx']}\")\n",
    "    print(f\"   Data at end: {event['data_at_end']:.4f} ± {event['std_at_end']:.4f}\")\n",
    "    print(f\"   Data 4 weeks after: {event['data_after']:.4f} ± {event['std_after']:.4f}\")\n",
    "    print(f\"   Delta data: {event['delta_data']:.4f}\")\n",
    "    print(f\"   Delta time (days): {event['delta_time']:.1f}\")\n",
    "    print(f\"   Gradient (per day): {event['gradient']:.6f} ± {event['gradient_error']:.6f}\")\n",
    "    print(f\"   Gradient (per week): {event['gradient_per_week']:.4f} ± {event['gradient_error_per_week']:.4f}\")\n",
    "    print(f\"   Expected gradient: ~0.017857 per day\")\n",
    "\n",
    "# Test without error computation\n",
    "print(\"\\n2. Testing gradient without error (error_window=0):\")\n",
    "result_no_error = angle_window_post_event_gradient(\n",
    "    test_time, test_angles, test_data,\n",
    "    angle_min=60, angle_max=70,\n",
    "    weeks_after=4,\n",
    "    error_window=0\n",
    ")\n",
    "\n",
    "if len(result_no_error) > 0:\n",
    "    event = result_no_error[0]\n",
    "    print(f\"   Gradient (per day): {event['gradient']:.6f}\")\n",
    "    print(f\"   Gradient error: {event['gradient_error']} (disabled)\")\n",
    "\n",
    "# Test with multiple events\n",
    "print(\"\\n3. Testing with multiple angle events:\")\n",
    "test_angles_multi = 90 + 10 * np.sin(2 * np.pi * test_time / 50)\n",
    "test_data_multi = np.linspace(0, 1, n_points)  # Steady increase\n",
    "\n",
    "result_multi = angle_window_post_event_gradient(\n",
    "    test_time, test_angles_multi, test_data_multi,\n",
    "    angle_min=88, angle_max=92,\n",
    "    weeks_after=2,\n",
    "    error_window=5\n",
    ")\n",
    "\n",
    "print(f\"   Found {len(result_multi)} events\")\n",
    "if len(result_multi) > 0:\n",
    "    for i, event in enumerate(result_multi[:3]):  # Show first 3 events\n",
    "        if not np.isnan(event['gradient']):\n",
    "            print(f\"   Event {i+1}: gradient = {event['gradient']:.6f} ± {event['gradient_error']:.6f} per day\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Test complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8uilfnsayeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual test: Plot to show what the functions are measuring\n",
    "print(\"Visual demonstration of post-event statistics:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create synthetic data\n",
    "n_points = 150\n",
    "start = 2426342.5\n",
    "test_time = start + np.arange(n_points)\n",
    "test_angles = np.linspace(0, 180, n_points)\n",
    "\n",
    "# Create data with a clear pattern after event end\n",
    "np.random.seed(42)\n",
    "test_data = 0.5 + np.random.normal(0, 0.05, n_points)\n",
    "\n",
    "# Event ends around index 50\n",
    "event_end = 50\n",
    "# 4 weeks after = index 78 (28 days later)\n",
    "weeks = 4\n",
    "after_idx = event_end + weeks * 7\n",
    "\n",
    "# Create lower values at event end\n",
    "test_data[event_end-3:event_end+3] = 0.25 + np.random.normal(0, 0.02, 6)\n",
    "\n",
    "# Create higher values 4 weeks after\n",
    "test_data[after_idx-3:after_idx+3] = 0.75 + np.random.normal(0, 0.02, 6)\n",
    "\n",
    "# Run the functions\n",
    "diff_result = angle_window_post_event_difference(\n",
    "    test_time, test_angles, test_data,\n",
    "    angle_min=60, angle_max=70,\n",
    "    weeks_after=4,\n",
    "    n_points_at_end=10,\n",
    "    n_points_after=10\n",
    ")\n",
    "\n",
    "grad_result = angle_window_post_event_gradient(\n",
    "    test_time, test_angles, test_data,\n",
    "    angle_min=60, angle_max=70,\n",
    "    weeks_after=4\n",
    ")\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Top panel: Show the angle and event window\n",
    "ax1.plot(test_time-start, test_angles, 'b-', linewidth=1.5, label='Angle')\n",
    "ax1.axhline(y=60, color='r', linestyle='--', alpha=0.5, label='Event boundaries')\n",
    "ax1.axhline(y=70, color='r', linestyle='--', alpha=0.5)\n",
    "if len(diff_result) > 0:\n",
    "    event_end_idx = diff_result[0]['end_idx']\n",
    "    ax1.axvline(x=event_end_idx, color='g', linestyle='--', linewidth=2, label='Event end')\n",
    "ax1.set_ylabel('Angle (degrees)')\n",
    "ax1.set_title('Angle Time Series (Events are when angle is between 60-70 degrees)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom panel: Show the data and measurements\n",
    "ax2.plot(test_time-start, test_data, 'k-', linewidth=1, alpha=0.6, label='Data')\n",
    "if len(diff_result) > 0:\n",
    "    event = diff_result[0]\n",
    "    end_idx = event['end_idx']\n",
    "    \n",
    "    # Mark event end\n",
    "    ax2.axvline(x=end_idx, color='g', linestyle='--', linewidth=2, label='Event end')\n",
    "    \n",
    "    # Mark 4 weeks after\n",
    "    ax2.axvline(x=end_idx + 28, color='purple', linestyle='--', linewidth=2, \n",
    "                label=f'{weeks} weeks after')\n",
    "    \n",
    "    # Show the mean values\n",
    "    ax2.plot(end_idx, event['mean_at_end'], 'go', markersize=12, \n",
    "            label=f\"Mean at end = {event['mean_at_end']:.3f}\")\n",
    "    ax2.plot(end_idx + 28, event['mean_after'], 'mo', markersize=12,\n",
    "            label=f\"Mean after = {event['mean_after']:.3f}\")\n",
    "    \n",
    "    # Draw arrow showing difference\n",
    "    ax2.annotate('', xy=(end_idx + 35, event['mean_after']), \n",
    "                xytext=(end_idx + 35, event['mean_at_end']),\n",
    "                arrowprops=dict(arrowstyle='<->', color='red', lw=2))\n",
    "    ax2.text(end_idx + 38, (event['mean_at_end'] + event['mean_after'])/2,\n",
    "            f\"Δ = {event['difference']:.3f}\\n± {event['error']:.3f}\",\n",
    "            fontsize=10, color='red')\n",
    "\n",
    "ax2.set_xlabel('Time (days)')\n",
    "ax2.set_ylabel('Data value')\n",
    "ax2.set_title('Data Time Series (showing post-event difference measurement)')\n",
    "ax2.legend(loc='upper left')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if len(diff_result) > 0 and len(grad_result) > 0:\n",
    "    print(f\"\\nResults summary:\")\n",
    "    print(f\"  Difference method: Δ = {diff_result[0]['difference']:.4f} ± {diff_result[0]['error']:.4f}\")\n",
    "    print(f\"  Gradient method: {grad_result[0]['gradient']:.6f} per day\")\n",
    "    print(f\"                   {grad_result[0]['gradient_per_week']:.4f} per week\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g19kxmjdjps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot post-event gradients with error bars\n",
    "# Assuming you have stored the gradient result in a variable (e.g., 'grad_result')\n",
    "# grad_result = angle_window_post_event_gradient(...)\n",
    "\n",
    "# Extract data from the result\n",
    "gradients = [event['gradient_per_week'] for event in grad_result]\n",
    "gradient_errors = [event['gradient_error_per_week'] for event in grad_result]\n",
    "event_times_jd = [start+event['end_time'] for event in grad_result]\n",
    "\n",
    "# Convert Julian dates to datetime\n",
    "event_dates = Time(event_times_jd, format='jd').to_datetime()\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Plot with error bars\n",
    "ax.errorbar(event_dates, gradients, yerr=gradient_errors, \n",
    "            fmt='s', markersize=8, capsize=5, capthick=2,\n",
    "            color='darkgreen', ecolor='lightgreen', elinewidth=2,\n",
    "            label='Gradient ± Error')\n",
    "\n",
    "# Add horizontal line at zero for reference\n",
    "ax.axhline(y=0, color='red', linestyle='--', linewidth=1.5, alpha=0.7, label='Zero gradient')\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel('Event End Date', fontsize=12)\n",
    "ax.set_ylabel('Gradient (per week)', fontsize=12)\n",
    "ax.set_title('Post-Event Gradient with Error Bars', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Format x-axis to show dates nicely\n",
    "import matplotlib.dates as mdates\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"Summary of {len(gradients)} events:\")\n",
    "print(f\"  Mean gradient: {np.nanmean(gradients):.6f} ± {np.nanstd(gradients):.6f} per week\")\n",
    "print(f\"  Median gradient: {np.nanmedian(gradients):.6f} per week\")\n",
    "print(f\"  Mean error: {np.nanmean(gradient_errors):.6f} per week\")\n",
    "print(f\"  Number of positive gradients: {sum(1 for g in gradients if g > 0)}\")\n",
    "print(f\"  Number of negative gradients: {sum(1 for g in gradients if g < 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "szzt7xk9a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot post-event differences with error bars\n",
    "# Assuming you have stored the result in a variable (e.g., 'diff_result')\n",
    "# diff_result = angle_window_post_event_difference(...)\n",
    "\n",
    "# Extract data from the result\n",
    "differences = [event['difference'] for event in diff_result]\n",
    "errors = [event['error'] for event in diff_result]\n",
    "event_times_jd = [event['end_time'] for event in diff_result]\n",
    "\n",
    "# Convert Julian dates to datetime\n",
    "event_dates = Time(event_times_jd, format='jd').to_datetime()\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Plot with error bars\n",
    "ax.errorbar(event_dates, differences, yerr=errors, \n",
    "            fmt='o', markersize=8, capsize=5, capthick=2,\n",
    "            color='steelblue', ecolor='lightsteelblue', elinewidth=2,\n",
    "            label='Difference ± Error')\n",
    "\n",
    "# Add horizontal line at zero for reference\n",
    "ax.axhline(y=0, color='red', linestyle='--', linewidth=1.5, alpha=0.7, label='Zero line')\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel('Event End Date', fontsize=12)\n",
    "ax.set_ylabel('Difference (After - At End)', fontsize=12)\n",
    "ax.set_title('Post-Event Difference in Data with Error Bars', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Format x-axis to show dates nicely\n",
    "import matplotlib.dates as mdates\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"Summary of {len(differences)} events:\")\n",
    "print(f\"  Mean difference: {np.nanmean(differences):.4f} ± {np.nanstd(differences):.4f}\")\n",
    "print(f\"  Median difference: {np.nanmedian(differences):.4f}\")\n",
    "print(f\"  Mean error: {np.nanmean(errors):.4f}\")\n",
    "print(f\"  Number of positive differences: {sum(1 for d in differences if d > 0)}\")\n",
    "print(f\"  Number of negative differences: {sum(1 for d in differences if d < 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0217ab5b",
   "metadata": {},
   "source": [
    "## Load HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a767fdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File(filePath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c17308a",
   "metadata": {},
   "source": [
    "## View relevant keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fc636b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Earth', 'Jupiter', 'Mercury', 'Neptune', 'Saturn', 'Sun', 'Uranus', 'Venus']\n",
      "<KeysViewHDF5 ['jd', 'vx', 'vy', 'vz', 'x', 'y', 'z']>\n",
      "<KeysViewHDF5 ['n_points', 'naif_id', 'units_position', 'units_velocity']>\n"
     ]
    }
   ],
   "source": [
    "planets = list(file.keys())\n",
    "print(planets)\n",
    "print(file['Earth'].keys())\n",
    "print(file['Earth'].attrs.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb200af",
   "metadata": {},
   "source": [
    "## Ensure date ranges of planetary ephemerides match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1d1831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataConsistent = True\n",
    "timearray = None\n",
    "for planet in planets:\n",
    "    if np.all(file[planet]['jd'][:] != file['Earth']['jd'][:]):\n",
    "        print(f\"Error as {planet}\")\n",
    "        dataConsistent = False\n",
    "if dataConsistent:\n",
    "    timearray = file['Earth']['jd'][:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca59f890",
   "metadata": {},
   "source": [
    "## Construct dictionary of 3D position values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00619ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ephemerides = {k:0 for k in planets}\n",
    "for planet in planets:\n",
    "    px = file[planet]['x'][:]*unit.AU.to(unit.m) # Meters\n",
    "    py = file[planet]['y'][:]*unit.AU.to(unit.m) # Meters\n",
    "    pz = file[planet]['z'][:]*unit.AU.to(unit.m) # Meters\n",
    "\n",
    "    coords = np.stack([px,py,pz], axis=1)\n",
    "\n",
    "    ephemerides[planet] = coords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5554107d",
   "metadata": {},
   "source": [
    "## Construct mass array corresponding to Planets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53fab5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "planet_masses = {\n",
    "    'Mercury': 3.3011e23, # kg\n",
    "    'Venus':4.8675e24, # kg\n",
    "    'Earth': 5.9724e24,# kg\n",
    "    #'Mars': 6.4171e23,# kg\n",
    "    'Jupiter':1.898e27,# kg\n",
    "    'Saturn':5.685e26,# kg\n",
    "    'Uranus':8.682e25,# kg\n",
    "    'Neptune':1.024e26# kg\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979cef23",
   "metadata": {},
   "source": [
    "## Plot representative of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97289e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = 400\n",
    "\n",
    "x,y = zip(*ephemerides[\"Earth\"][:max_index,0:2])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.set_xlim(min(x) - .2e11, max(x) + .2e11)\n",
    "ax.set_ylim(min(y) - .2e11, max(y) + .2e11)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(\"Earth Trajectory\")\n",
    "ax.grid(True)\n",
    "plt.plot(x,y, 'go', markersize=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02910680",
   "metadata": {},
   "source": [
    "# Barycenter Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b53df52",
   "metadata": {},
   "source": [
    "## Compute Barycenter for each planet group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d5b2108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Group1 total mass: 2.47767001e+27   Group2 total mass: 2.65572e+27\n"
     ]
    }
   ],
   "source": [
    "group1 = ['Mercury', 'Venus', 'Earth', 'Jupiter', 'Saturn']\n",
    "#group1 = ['Mercury', 'Venus', 'Earth']\n",
    "\n",
    "group2 = ['Jupiter', 'Saturn', 'Uranus', 'Neptune']\n",
    "\n",
    "\n",
    "# Group1 total mass\n",
    "g1_masses = np.asarray([planet_masses[p] for p in group1])\n",
    "\n",
    "# Group2 total mass\n",
    "g2_masses = np.asarray([planet_masses[p] for p in group2])\n",
    "\n",
    "# Compute weights for position vectors\n",
    "#g1_weights = g1_masses / (solar_mass + np.sum(g1_masses))\n",
    "#g2_weights = g2_masses / (solar_mass + np.sum(g2_masses))\n",
    "g1_weights = np.array([.14,.34,.15,.35,.02])\n",
    "g2_weights = np.array([.35,.40,.125,.125])\n",
    "\n",
    "print(f\" Group1 total mass: {np.sum(g1_masses)}   Group2 total mass: {np.sum(g2_masses)}\")\n",
    "\n",
    "# Compute total number of time series points\n",
    "n_points = len(timearray)\n",
    "\n",
    "# Compute the group1 barycenter\n",
    "g1_ephemerides = np.asarray([ephemerides[p] for p in group1])\n",
    "g1_barycenter = np.sum(g1_weights[:,None,None] * g1_ephemerides, axis=0)\n",
    "\n",
    "# Compute the group2 barycenter\n",
    "g2_ephemerides = np.asarray([ephemerides[p] for p in group2])\n",
    "g2_barycenter = np.sum(g2_weights[:,None,None] * g2_ephemerides, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689dfee7",
   "metadata": {},
   "source": [
    "## Compute angle between barycenters over the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8508bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner products\n",
    "dot_prods = np.einsum('ij,ij->i', g1_barycenter, g2_barycenter)\n",
    "norms = np.linalg.norm(g1_barycenter, axis=1) * np.linalg.norm(g2_barycenter, axis=1)\n",
    "\n",
    "# Compute the cosine of angles\n",
    "cos_angles = dot_prods / norms\n",
    "angles = np.arccos(cos_angles) * 180 / np.pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da69507f",
   "metadata": {},
   "source": [
    "## Compute plot of angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea5e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "tarr = Time(timearray, format='jd').to_datetime()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(tarr, angles, lw=1.5)\n",
    "plt.axhline(y=90, color='r', linestyle='--', linewidth=1.5, label='90 Degrees')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Angle (degrees)\")\n",
    "plt.title(\"New Weighted Angles Over Time\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Format date axis nicely\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "plt.gca().xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"../figures/barycenter_angles.png\")\n",
    "#plt.savefig('../literature/our_work/barycenter_angles.png')\n",
    "plt.savefig(\"../figures/weighted_angles.png\")\n",
    "plt.savefig('../literature/our_work/weighted_angles.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacfeea5",
   "metadata": {},
   "source": [
    "# Tidal Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a71163",
   "metadata": {},
   "source": [
    "## Tidal field study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e996368",
   "metadata": {},
   "source": [
    "### Group 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c141e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the tidal tensor for just Group 1 planetary bodies\n",
    "\n",
    "r_norm = np.linalg.norm(g1_ephemerides, axis=-1, keepdims=True)\n",
    "r_hat = g1_ephemerides / r_norm\n",
    "\n",
    "# outer product\n",
    "rhat_outer = r_hat[...,:,None] * r_hat[...,None,:] # ( n_planets, n_time_points, 3, 3)\n",
    "\n",
    "# Identity\n",
    "I = np.eye(3)[None, None, :,:]\n",
    "\n",
    "# per-body tidal tensor:\n",
    "inv_r3 = (1.0 / r_norm**3)[...,None]\n",
    "m_b = g1_masses[:,None,  None,None]\n",
    "\n",
    "T_k = cons.G.value * m_b * (3.0 * rhat_outer - I) * inv_r3\n",
    "\n",
    "g1_T = T_k.sum(axis=0)\n",
    "g1_T_mag = np.sqrt((g1_T**2).sum(axis=(-2,-1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58742223",
   "metadata": {},
   "source": [
    "### Group 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d0c797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the tidal tensor for just Group 2 planetary bodies\n",
    "\n",
    "r_norm = np.linalg.norm(g2_ephemerides, axis=-1, keepdims=True)\n",
    "r_hat = g2_ephemerides / r_norm\n",
    "\n",
    "# outer product\n",
    "rhat_outer = r_hat[...,:,None] * r_hat[...,None,:] # ( n_planets, n_time_points, 3, 3)\n",
    "\n",
    "# Identity\n",
    "I = np.eye(3)[None, None, :,:]\n",
    "\n",
    "# per-body tidal tensor:\n",
    "inv_r3 = (1.0 / r_norm**3)[...,None]\n",
    "m_b = g2_masses[:,None,  None,None]\n",
    "\n",
    "T_k = cons.G.value * m_b * (3.0 * rhat_outer - I) * inv_r3\n",
    "\n",
    "g2_T = T_k.sum(axis=0)\n",
    "g2_T_mag = np.sqrt((g2_T**2).sum(axis=(-2,-1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4611438",
   "metadata": {},
   "source": [
    "### Total solar system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a67142",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_eph = np.asarray([ephemerides[p] for p in planet_masses.keys()])\n",
    "planet_mass = np.asarray([planet_masses[p] for p in planet_masses.keys()])\n",
    "r_norm = np.linalg.norm(total_eph, axis=-1, keepdims=True)\n",
    "r_hat = total_eph / r_norm\n",
    "\n",
    "# outer product\n",
    "rhat_outer = r_hat[...,:,None] * r_hat[...,None,:] # ( n_planets, n_time_points, 3, 3)\n",
    "\n",
    "# Identity\n",
    "I = np.eye(3)[None, None, :,:]\n",
    "\n",
    "# per-body tidal tensor:\n",
    "inv_r3 = (1.0 / r_norm**3)[...,None]\n",
    "m_b = planet_mass[:,None,  None,None]\n",
    "\n",
    "T_k = cons.G.value * m_b * (3.0 * rhat_outer - I) * inv_r3\n",
    "\n",
    "ss_T = T_k.sum(axis=0)\n",
    "ss_T_mag = np.sqrt((ss_T**2).sum(axis=(-2,-1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72fb782",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "tarr = Time(timearray, format='jd').to_datetime()\n",
    "\n",
    "ax.plot(tarr, ss_T_mag, color='dodgerblue', linewidth=2)\n",
    "ax.set_xlabel(\"Date\", fontsize=12)\n",
    "ax.set_ylabel(r\"Tidal tensor magnitude  $|T|$  (s$^{-2}$)\", fontsize=12)\n",
    "ax.set_title(\"Solar-System Tidal Tensor Magnitude at the Sun’s Center\", fontsize=13)\n",
    "\n",
    "# Optional: format the time axis nicely\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "plt.gca().xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "ax.grid(True, which='both', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../figures/full_tidal_tensor_mag.png\")\n",
    "plt.savefig('../literature/our_work/full_tidal_tensor_mag.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e5b53d",
   "metadata": {},
   "source": [
    "## Compare angles to tidal tensor magnitude at sun location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26fc0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "circ_lin_corr(angles, ss_T_mag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e500ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal as signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = ss_T_mag\n",
    "y = np.cos(angles)\n",
    "\n",
    "# Remove mean / linear trend to avoid huge low-freq leakage\n",
    "x = signal.detrend(x - np.mean(x))\n",
    "y = signal.detrend(y - np.mean(y))\n",
    "\n",
    "fs = 1.0 / np.median(np.diff(timearray).astype('timedelta64[D]').astype(float))  # samples/day\n",
    "fs = fs * 365.25\n",
    "print(f\"Sampling rate ≈ {fs:.3f} samples/yr\")\n",
    "\n",
    "f, Cxy = signal.coherence(x, y, fs=fs,\n",
    "                          nperseg=len(x)//8,\n",
    "                          noverlap=len(x)//16,\n",
    "                          window='hann')\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.semilogx(f, Cxy, color='darkorange', lw=2)\n",
    "plt.xlabel(\"Frequency (1/yr)\")\n",
    "plt.ylabel(\"Coherence  $C_{xy}(f)$\")\n",
    "plt.title(\"Magnitude - Angle Coherence Spectrum\")\n",
    "plt.grid(True, which='both', alpha=0.3)\n",
    "\n",
    "# Annotate known orbital periods\n",
    "for P,label in [(11.86,\"Jupiter\"), (19.86,\"J-S synodic\")]:\n",
    "    plt.axvline(1/P, color='gray', ls='--')\n",
    "    plt.text(1/P, 0.46, label, rotation=90, va='center', ha='right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad263ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dt = np.median(np.diff(timearray).astype('timedelta64[D]').astype(float)) / 365.25  # years/sample\n",
    "WCT, aWCT, coi, freq, sig = wavelet.wct(x, y, dt, dj=0.125, s0=2*dt, J=int(9/0.125), sig=None)\n",
    "plt.figure(figsize=(10,5))\n",
    "T, F = np.meshgrid(timearray, 1/freq)\n",
    "plt.contourf(T, F, WCT, levels=np.linspace(0,1,50), cmap='viridis')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Period (years)')\n",
    "plt.xlabel('Date')\n",
    "plt.title('Wavelet Coherence |T| vs. cos(θ)')\n",
    "plt.colorbar(label='Coherence')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7511a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "WCT, aWCT, coi, freq, sig = wavelet.wct(\n",
    "    x, y,\n",
    "    dt,\n",
    "    dj=0.125,\n",
    "    s0=2*dt,\n",
    "    J=int(9/0.125),\n",
    "    significance_level=0.95,   # ask pycwt to compute significance\n",
    "    wavelet='morlet',\n",
    "    normalize=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5189958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "T, F = np.meshgrid(timearray, 1/freq)\n",
    "\n",
    "# main coherence field\n",
    "plt.contourf(T, F, WCT, levels=np.linspace(0, 1, 50), cmap='viridis')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Period (years)')\n",
    "plt.xlabel('Date')\n",
    "plt.title('Wavelet Coherence |T| vs. cos(θ)')\n",
    "plt.colorbar(label='Coherence')\n",
    "\n",
    "sig2d = np.tile(sig[:, None], (1, WCT.shape[1]))\n",
    "plt.contour(T, F, sig2d, levels=[0.95], colors='white', linewidths=1.2)\n",
    "\n",
    "print(f\" Any significant peridoicities? {np.any(sig2d>0.95)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0082b71",
   "metadata": {},
   "source": [
    "# Sunspot Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0d2005",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68785f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Decimal_Year</th>\n",
       "      <th>N_Sunspots</th>\n",
       "      <th>Std</th>\n",
       "      <th>N_Observations</th>\n",
       "      <th>Def/Prov</th>\n",
       "      <th>N_Sunspots_Norm</th>\n",
       "      <th>Tranquility</th>\n",
       "      <th>jd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1818</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1818.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2385070.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1818</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1818.004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2385071.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1818</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1818.007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2385072.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1818</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1818.010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2385073.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1818</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1818.012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2385074.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75905</th>\n",
       "      <td>2025</td>\n",
       "      <td>10</td>\n",
       "      <td>27</td>\n",
       "      <td>2025.821</td>\n",
       "      <td>98.0</td>\n",
       "      <td>17.2</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.185606</td>\n",
       "      <td>0.814394</td>\n",
       "      <td>2460975.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75906</th>\n",
       "      <td>2025</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>2025.823</td>\n",
       "      <td>114.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.215909</td>\n",
       "      <td>0.784091</td>\n",
       "      <td>2460976.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75907</th>\n",
       "      <td>2025</td>\n",
       "      <td>10</td>\n",
       "      <td>29</td>\n",
       "      <td>2025.826</td>\n",
       "      <td>100.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.189394</td>\n",
       "      <td>0.810606</td>\n",
       "      <td>2460977.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75908</th>\n",
       "      <td>2025</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>2025.829</td>\n",
       "      <td>70.0</td>\n",
       "      <td>15.7</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.132576</td>\n",
       "      <td>0.867424</td>\n",
       "      <td>2460978.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75909</th>\n",
       "      <td>2025</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>2025.832</td>\n",
       "      <td>44.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>2460979.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75910 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Year  Month  Day  Decimal_Year  N_Sunspots   Std  N_Observations  \\\n",
       "0      1818      1    1      1818.001         NaN   NaN               0   \n",
       "1      1818      1    2      1818.004         NaN   NaN               0   \n",
       "2      1818      1    3      1818.007         NaN   NaN               0   \n",
       "3      1818      1    4      1818.010         NaN   NaN               0   \n",
       "4      1818      1    5      1818.012         NaN   NaN               0   \n",
       "...     ...    ...  ...           ...         ...   ...             ...   \n",
       "75905  2025     10   27      2025.821        98.0  17.2              31   \n",
       "75906  2025     10   28      2025.823       114.0  13.6              30   \n",
       "75907  2025     10   29      2025.826       100.0  13.3              23   \n",
       "75908  2025     10   30      2025.829        70.0  15.7              25   \n",
       "75909  2025     10   31      2025.832        44.0  11.3              22   \n",
       "\n",
       "       Def/Prov  N_Sunspots_Norm  Tranquility         jd  \n",
       "0             1              NaN          NaN  2385070.5  \n",
       "1             1              NaN          NaN  2385071.5  \n",
       "2             1              NaN          NaN  2385072.5  \n",
       "3             1              NaN          NaN  2385073.5  \n",
       "4             1              NaN          NaN  2385074.5  \n",
       "...         ...              ...          ...        ...  \n",
       "75905         0         0.185606     0.814394  2460975.5  \n",
       "75906         0         0.215909     0.784091  2460976.5  \n",
       "75907         0         0.189394     0.810606  2460977.5  \n",
       "75908         0         0.132576     0.867424  2460978.5  \n",
       "75909         0         0.083333     0.916667  2460979.5  \n",
       "\n",
       "[75910 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Path to sunspots data file\n",
    "__SUNSPOT_FILE__ = Path(\"../data/SN_d_tot_V2.0.csv\")\n",
    "\n",
    "# Load the csv file into memory\n",
    "sp_file =pd.read_csv(__SUNSPOT_FILE__, sep = None, na_values=['-1', 'NaN'], engine='python')\n",
    "\n",
    "# Compute the normalized number of sunspots, ranging from 0 to 1\n",
    "minv = sp_file['N_Sunspots'].min()\n",
    "maxv = sp_file['N_Sunspots'].max()\n",
    "mean = sp_file['N_Sunspots'].mean()\n",
    "std = sp_file['N_Sunspots'].std()\n",
    "sp_file['N_Sunspots_Norm'] = (sp_file['N_Sunspots'] - minv) / (maxv - minv)\n",
    "\n",
    "# Compute the solar tranquility, defined by 1 - N_Sunspots_Norm\n",
    "sp_file[\"Tranquility\"] = (1 - sp_file[\"N_Sunspots_Norm\"])\n",
    "\n",
    "# Create new column converting Year-Month-Day data into julian date\n",
    "sp_file['jd'] = Time(\n",
    "    [f\"{y}-{m}-{d}\" for y, m, d in zip(sp_file['Year'], sp_file['Month'], sp_file['Day'])],\n",
    "    format='iso'\n",
    ").jd\n",
    "\n",
    "# Print summary of datatable\n",
    "sp_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "936ccf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34638 (34698,)\n"
     ]
    }
   ],
   "source": [
    "# Grab only the dates that correspond to our selected epochs\n",
    "\n",
    "# Generate mask for each set of dates\n",
    "timearray_mask = np.isin(timearray, sp_file['jd'].values)\n",
    "sp_mask = np.isin(sp_file['jd'].values, timearray)\n",
    "\n",
    "# Slice the dataframe according to the timearray dates\n",
    "sliced_df = sp_file[sp_file['jd'].isin(timearray)]\n",
    "sliced_df\n",
    "\n",
    "print(sliced_df.shape[0], timearray.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fed8291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates are aligned: True\n",
      "Data is consistent lengths: True\n"
     ]
    }
   ],
   "source": [
    "## Slice the angles according to the date range of the sunspots\n",
    "sliced_angles = angles[timearray_mask]\n",
    "\n",
    "## Extract sunspots for from sliced dateframe\n",
    "sunspots = sliced_df['N_Sunspots_Norm'].values\n",
    "\n",
    "## Ensure dates are aligned\n",
    "print(f\"Dates are aligned: {np.all(sliced_df['jd'] == timearray[timearray_mask])}\")\n",
    "\n",
    "## Ensure data lists are identical lengths\n",
    "print(f\"Data is consistent lengths: {len(sliced_angles) == len(sunspots)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9fce55",
   "metadata": {},
   "source": [
    "## Compute Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566fea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute correlation and p-score\n",
    "circ_lin_corr(sliced_angles, sunspots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559799cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = np.arange(-120,120,10)\n",
    "out = sweep_lag_circ_lin(timearray[timearray_mask], sliced_angles, sliced_df['jd'], sunspots, lags, L=30, n_perm=2000)\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9154e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = wavelet_cross_analysis(sliced_angles, sunspots, timearray[1] - timearray[0])\n",
    "print(f\"Mean coherence: {np.nanmean(res['wtc']):.3f}\")\n",
    "print(f\"Mean delay: {np.nanmean(res['delay']):.2f} time units\")\n",
    "res.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9400dfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- align array lengths ---\n",
    "time = np.asarray(timearray[timearray_mask], float)\n",
    "period = np.asarray(res[\"period\"], float)\n",
    "wtc = np.asarray(res[\"wtc\"])\n",
    "angles_deg = sliced_angles\n",
    "\n",
    "n = min(len(time), wtc.shape[1], len(angles_deg))\n",
    "time, wtc, angles_deg = time[:n], wtc[:, :n], angles_deg[:n]\n",
    "\n",
    "# --- build figure with tight, touching layout ---\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "gs = fig.add_gridspec(2, 1, height_ratios=[3, 1], hspace=0.05)\n",
    "ax_top = fig.add_subplot(gs[0])\n",
    "ax_bottom = fig.add_subplot(gs[1], sharex=ax_top)\n",
    "\n",
    "# --- top: wavelet coherence ---\n",
    "T, P = np.meshgrid(time, period)\n",
    "pcm = ax_top.pcolormesh(T, P, wtc, shading=\"auto\", cmap=\"turbo\")\n",
    "ax_top.set_yscale(\"log\")\n",
    "ax_top.set_ylabel(\"Period\")\n",
    "ax_top.set_title(\"Wavelet Coherence\")\n",
    "fig.colorbar(pcm, ax=ax_top, orientation=\"vertical\", label=\"Coherence\")\n",
    "\n",
    "# hide top plot’s x ticks and labels\n",
    "ax_top.tick_params(labelbottom=False)\n",
    "ax_top.set_xlabel(\"\")  # no label on top plot\n",
    "\n",
    "# --- bottom: driver angle vs. time ---\n",
    "ax_bottom.plot(time, angles_deg, lw=0.8, color=\"tab:blue\")\n",
    "ax_bottom.set_ylabel(\"Angle (°)\")\n",
    "ax_bottom.set_xlabel(\"Time (Julian Date)\")\n",
    "ax_bottom.set_xlim(time[0], time[-1])\n",
    "#ax_bottom.set_title(\"Driver Angle vs. Time\")\n",
    "\n",
    "# --- ensure exact horizontal alignment ---\n",
    "pos0 = ax_top.get_position()\n",
    "pos1 = ax_bottom.get_position()\n",
    "ax_bottom.set_position([pos0.x0, pos1.y0, pos0.width, pos1.height])\n",
    "\n",
    "\n",
    "plt.savefig(\"../figures/wavelet_coh.png\")\n",
    "plt.savefig('../literature/our_work/wavelet_coh.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a4dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_raw = np.angle(res['Wxy'])\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- align array lengths ---\n",
    "time = np.asarray(timearray[timearray_mask], float)\n",
    "period = np.asarray(res[\"period\"], float)\n",
    "wtc = phase_raw\n",
    "angles_deg = sliced_angles\n",
    "\n",
    "n = min(len(time), wtc.shape[1], len(angles_deg))\n",
    "time, wtc, angles_deg = time[:n], wtc[:, :n], angles_deg[:n]\n",
    "\n",
    "# --- build figure with tight, touching layout ---\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "gs = fig.add_gridspec(2, 1, height_ratios=[3, 1], hspace=0.05)\n",
    "ax_top = fig.add_subplot(gs[0])\n",
    "ax_bottom = fig.add_subplot(gs[1], sharex=ax_top)\n",
    "\n",
    "# --- top: wavelet coherence ---\n",
    "T, P = np.meshgrid(time, period)\n",
    "pcm = ax_top.pcolormesh(T, P, wtc, shading=\"auto\", cmap=\"turbo\")\n",
    "ax_top.set_yscale(\"log\")\n",
    "ax_top.set_ylabel(\"Period\")\n",
    "ax_top.set_title(\"Relative Phase\")\n",
    "fig.colorbar(pcm, ax=ax_top, orientation=\"vertical\", label=\"Coherence\")\n",
    "\n",
    "# hide top plot’s x ticks and labels\n",
    "ax_top.tick_params(labelbottom=False)\n",
    "ax_top.set_xlabel(\"\")  # no label on top plot\n",
    "\n",
    "# --- bottom: driver angle vs. time ---\n",
    "ax_bottom.plot(time, angles_deg, lw=0.8, color=\"tab:blue\")\n",
    "ax_bottom.set_ylabel(\"Angle (°)\")\n",
    "ax_bottom.set_xlabel(\"Time (Julian Date)\")\n",
    "ax_bottom.set_xlim(time[0], time[-1])\n",
    "#ax_bottom.set_title(\"Driver Angle vs. Time\")\n",
    "\n",
    "# --- ensure exact horizontal alignment ---\n",
    "pos0 = ax_top.get_position()\n",
    "pos1 = ax_bottom.get_position()\n",
    "ax_bottom.set_position([pos0.x0, pos1.y0, pos0.width, pos1.height])\n",
    "\n",
    "plt.savefig(\"../figures/wavelet_coh_phase.png\")\n",
    "plt.savefig('../literature/our_work/wavelet_coh_phase.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bc9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tarr = Time(timearray[timearray_mask], format='jd').to_datetime()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(tarr, sunspots, lw=1.5)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Tranquility\")\n",
    "plt.title(\"Temporal Evolution of Solar Tranquility\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Format date axis nicely\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "plt.gca().xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../figures/soltranq.png\")\n",
    "plt.savefig('../literature/our_work/soltranq.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9c1e82",
   "metadata": {},
   "source": [
    "# Comparison of Coordinate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28805d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merctime =file[\"Mercury\"]['jd'][:]\n",
    "mercx = file['Mercury']['x'][:]\n",
    "mercz = file['Mercury']['z'][:]\n",
    "tarr = Time(merctime, format='jd').to_datetime()\n",
    "\n",
    "# The below values agree with the Horizons website API query\n",
    "print(tarr[1464], mercx[1464])\n",
    "print(tarr[1464], mercz[1464])\n",
    "print(tarr[1465], mercx[1465])\n",
    "print(tarr[1465], mercz[1465])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7c7bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b809c2f3",
   "metadata": {},
   "source": [
    "# 90 Degree Events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2064d61f",
   "metadata": {},
   "source": [
    "## Study A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87cc0c3",
   "metadata": {},
   "source": [
    "### Figure 1 Recreation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc59d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, recreate Fig. 1\n",
    "\n",
    "tranquility = sp_file['Tranquility']\n",
    "\n",
    "time_lower = Time('2015-06-05', format='iso').jd\n",
    "time_higher = Time('2019-12-05', format='iso').jd\n",
    "\n",
    "# Generate the time mask\n",
    "MASK = ( timearray >= time_lower ) & ( timearray <= time_higher)\n",
    "\n",
    "# Slice the time array\n",
    "timearray = file['Earth']['jd'][:]\n",
    "timearray_sliced = timearray[ MASK ]\n",
    "\n",
    "# Slice the angle array\n",
    "angles_sliced = angles[ MASK ]\n",
    "\n",
    "# Create a boolean array to show where 90 degree events occur\n",
    "events_bool = ( angles_sliced >= __ANGLE_LIMIT_LOWER__ ) & ( angles_sliced <= __ANGLE_LIMIT_HIGHER__)\n",
    "\n",
    "# SLice the tranquility array to the times as well\n",
    "sp_mask = ( sp_file['jd'] >= time_lower ) & ( sp_file['jd'] <= time_higher )\n",
    "tranq_masked = tranquility[sp_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84310bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = Time(timearray_sliced, format='jd')\n",
    "iso_dates = times.iso\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(timearray_sliced, 0.8 * events_bool, 'b-', linewidth=1.5, label='90 degree events') # Rescaled for nice figure\n",
    "ax.plot(timearray_sliced, tranq_masked, 'r-', linewidth=1.5, label='solar tranquility')\n",
    "\n",
    "# Set only 10 x-ticks with ISO format labels\n",
    "tick_indices = np.linspace(0, len(timearray_sliced)-1, 10, dtype=int)\n",
    "ax.set_xticks(timearray_sliced[tick_indices])\n",
    "ax.set_xticklabels([iso_dates[i][:10] for i in tick_indices], rotation=45, ha='right')\n",
    "\n",
    "ax.set_xlabel('Date (ISO)')\n",
    "ax.get_yaxis().set_visible(False)  # Turn off y-axis\n",
    "ax.set_title('Solar Tranquility versus 90 degree events')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.savefig(\"../figures/sol_tranq_and_90_degree_select.png\")\n",
    "plt.savefig('../literature/our_work/sol_tranq_and_90_degree_select.png')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d51ec5c",
   "metadata": {},
   "source": [
    "### Figure 2 Recreation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e9fff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tranquility = sp_file['Tranquility']\n",
    "\n",
    "time_lower = Time('1935-01-11', format='iso').jd\n",
    "time_higher = Time('2022-01-11', format='iso').jd\n",
    "\n",
    "# Generate the time mask\n",
    "MASK = ( timearray >= time_lower ) & ( timearray <= time_higher)\n",
    "\n",
    "# Slice the time array\n",
    "timearray = file['Earth']['jd'][:]\n",
    "timearray_sliced = timearray[ MASK ]\n",
    "\n",
    "# Slice the angle array\n",
    "angles_sliced = angles[ MASK ]\n",
    "\n",
    "# Create a boolean array to show where 90 degree events occur\n",
    "events_bool = ( angles_sliced >= __ANGLE_LIMIT_LOWER__ ) & ( angles_sliced <= __ANGLE_LIMIT_HIGHER__)\n",
    "\n",
    "# SLice the tranquility array to the times as well\n",
    "sp_mask = ( sp_file['jd'] >= time_lower ) & ( sp_file['jd'] <= time_higher )\n",
    "tranq_masked = tranquility[sp_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698d4cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = Time(timearray_sliced, format='jd')\n",
    "iso_dates = times.iso\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(timearray_sliced, 2 * events_bool, 'b-', linewidth=1.5, label='90 degree events') # Rescaled for nice figure\n",
    "ax.plot(timearray_sliced, tranq_masked, 'r-', linewidth=1.5, label='solar tranquility', alpha=0.8)\n",
    "\n",
    "# Set only 10 x-ticks with ISO format labels\n",
    "tick_indices = np.linspace(0, len(timearray_sliced)-1, 10, dtype=int)\n",
    "ax.set_xticks(timearray_sliced[tick_indices])\n",
    "ax.set_xticklabels([iso_dates[i][:10] for i in tick_indices], rotation=45, ha='right')\n",
    "\n",
    "ax.set_xlabel('Date (ISO)')\n",
    "ax.get_yaxis().set_visible(False)  # Turn off y-axis\n",
    "ax.set_title('Solar Tranquility versus 90 degree events')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.savefig(\"../figures/sol_tranq_and_90_degree.png\")\n",
    "plt.savefig('../literature/our_work/sol_tranq_and_90_degree.png')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b6e8b6",
   "metadata": {},
   "source": [
    "### Stability of Solar Tranquility Figures of Merit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b71bd4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 7-day rolling average of Tranquility\n",
      "Time arrays are aligned: True\n"
     ]
    }
   ],
   "source": [
    "tranquility = sp_file['Tranquility']\n",
    "\n",
    "print(f\"Using 7-day rolling average of Tranquility\")\n",
    "tranquility = tranquility.rolling(window=7).mean()\n",
    "\n",
    "\n",
    "sp_file_time = sp_file['jd'].values\n",
    "ps_file_time = file['Earth']['jd'][:]\n",
    "\n",
    "# Use entire aligned date range\n",
    "time_lower = np.max([sp_file_time[0], ps_file_time[0]]) #Time('2015-06-05', format='iso').jd\n",
    "time_higher = np.min([sp_file_time[-1], ps_file_time[-1]]) # Time('2019-12-05', format='iso').jd\n",
    "\n",
    "# Generate the time mask\n",
    "MASK = ( timearray >= time_lower ) & ( timearray <= time_higher)\n",
    "SP_MASK = ( sp_file['jd'] >= time_lower ) & ( sp_file['jd'] <= time_higher )\n",
    "\n",
    "# Slice the time array\n",
    "\n",
    "timearray_sliced = timearray[ MASK ]\n",
    "sp_time_sliced = sp_file_time[ SP_MASK ]\n",
    "\n",
    "# Ensure time arrays are aligned\n",
    "print(f\"Time arrays are aligned: {np.all(timearray_sliced == sp_time_sliced)}\")\n",
    "\n",
    "# Slice the angle array\n",
    "angles_sliced = angles[ MASK ]\n",
    "\n",
    "# Create a boolean array to show where 90 degree events occur\n",
    "events_bool = ( angles_sliced >= __ANGLE_LIMIT_LOWER__ ) & ( angles_sliced <= __ANGLE_LIMIT_HIGHER__)\n",
    "\n",
    "# SLice the tranquility array to the times as well\n",
    "tranq_masked = tranquility[SP_MASK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "358e12c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events: 26\n",
      "Ratios: [np.float64(0.8254108436214738), np.float64(2.0745294113695785), np.float64(0.8156184951322891), np.float64(0.2325840194880084), np.float64(0.13921575702875308), np.float64(0.10735958008315599), np.float64(0.2839853202648998), np.float64(0.6678476402844529), np.float64(1.0246720616179017), np.float64(0.7233846977208683), np.float64(3.9742857206368365), np.float64(0.5177228679056024), np.float64(0.8335416826869785), np.float64(0.03658280366665759), np.float64(0.371707108282157), np.float64(0.22645249253470257), np.float64(0.9288803623406717), np.float64(0.7221644497270191), np.float64(0.4182991565650158), np.float64(1.754362369560569), np.float64(0.3340432263776896), np.float64(0.6122539603848873), np.float64(0.6024761565610376), np.float64(0.5980541738910763), np.float64(0.058560533187764156), np.float64(0.6568202475867952)]\n",
      "Mean: 0.751569813019494\n"
     ]
    }
   ],
   "source": [
    "# Compute complete running variance ratio of solar tranquility\n",
    "\n",
    "tranq_var_rat_pre = angle_window_variance_ratio(\n",
    "    timearray_sliced,\n",
    "    angles_sliced,\n",
    "    tranq_masked,\n",
    "    86,\n",
    "    94,\n",
    "    12*7,\n",
    "    inverse = False\n",
    ")\n",
    "\n",
    "# Drop NaN values (occurs usually when there is no sunspot data for that date)\n",
    "tranq_var_rat_pre_filt = [win for win in tranq_var_rat_pre if not np.isnan(win['ratio']) ]\n",
    "rats_pre = [win['ratio'] for win in tranq_var_rat_pre]\n",
    "print(f\"Number of events: {len(tranq_var_rat_pre_filt)}\")\n",
    "print(f\"Ratios: {rats_pre}\")\n",
    "print(f\"Mean: {np.nanmean(rats_pre)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "402846a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "starts = [win['start_idx'] for win in tranq_var_rat_pre]\n",
    "ends = [win['end_idx'] for win in tranq_var_rat_pre]\n",
    "mindiff = np.inf\n",
    "for st in starts:\n",
    "    for en in ends:\n",
    "        if np.abs(en-st)<mindiff:\n",
    "            mindiff = np.abs(en-st)\n",
    "\n",
    "mindiff * np.diff(timearray_sliced)[0]\n",
    "\n",
    "n_close_events = 0\n",
    "for st in starts:\n",
    "    for en in ends:\n",
    "        if np.abs(en-st) < 4*7:\n",
    "            n_close_events += 1\n",
    "print(n_close_events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f1037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(rats_pre)\n",
    "ax.axhline(y=1, label='unity', color='red', linestyle='--');\n",
    "ax.set_title(\"Pre-During Variance with 14 day lookback\")\n",
    "ax.set_ylabel(\"Var(during) / Var(Pre)\")\n",
    "ax.set_xlabel(\"Event Index\")\n",
    "ax.grid(True)\n",
    "plt.legend();\n",
    "\n",
    "plt.savefig(\"../figures/sol_tranq_pre_var.png\")\n",
    "plt.savefig('../literature/our_work/sol_tranq_pre_var.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194b52be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tranq_var_rat_post = angle_window_variance_ratio_post(\n",
    "    timearray_sliced,\n",
    "    angles_sliced,\n",
    "    tranq_masked,\n",
    "    86,\n",
    "    94,\n",
    "    12*7,\n",
    "    inverse = False\n",
    ")\n",
    "\n",
    "# Drop NaN values (occurs usually when there is no sunspot data for that date)\n",
    "tranq_var_rat_post_filt = [win for win in tranq_var_rat_post if not np.isnan(win['ratio']) ]\n",
    "rats_post = [win['ratio'] for win in tranq_var_rat_post][:]\n",
    "rats_post = np.delete(rats_post,5)\n",
    "print(f\"Number of events: {len(tranq_var_rat_post_filt)}\")\n",
    "print(f\"Ratios: {rats_post}\")\n",
    "print(f\"Mean: {np.nanmean(rats_post)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf41ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(rats_post)\n",
    "ax.axhline(y=1, label='unity', color='red', linestyle='--');\n",
    "ax.set_title(\"During/Post Variance with 14 day lookforward\")\n",
    "ax.set_ylabel(\"Var(during) / Var(Post)\")\n",
    "ax.set_xlabel(\"Event Index\")\n",
    "ax.grid(True)\n",
    "ax.set_ylim(0,np.max(rats_post))\n",
    "plt.legend();\n",
    "plt.savefig(\"../figures/sol_tranq_post_var.png\")\n",
    "plt.savefig('../literature/our_work/sol_tranq_post_var.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f68d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean pre ratio versus lookback time\n",
    "\n",
    "def compute_pre_mean(lb_time):\n",
    "    res = angle_window_variance_ratio(\n",
    "        timearray_sliced,\n",
    "        angles_sliced,\n",
    "        tranq_masked,\n",
    "        86,\n",
    "        94,\n",
    "        lb_time,\n",
    "        inverse = False\n",
    "    )\n",
    "\n",
    "    # Drop NaN values (occurs usually when there is no sunspot data for that date)\n",
    "    rats_pre = [win['ratio'] for win in res]\n",
    "    return np.mean(rats_pre)\n",
    "\n",
    "def compute_post_mean(lb_time):\n",
    "    res = angle_window_variance_ratio_post(\n",
    "        timearray_sliced,\n",
    "        angles_sliced,\n",
    "        tranq_masked,\n",
    "        86,\n",
    "        94,\n",
    "        lb_time,\n",
    "        inverse = False\n",
    "    )\n",
    "\n",
    "    # Drop NaN values (occurs usually when there is no sunspot data for that date)\n",
    "    rats_pre = [win['ratio'] for win in res]\n",
    "    return np.nanmean(rats_pre)\n",
    "\n",
    "pre_means = [compute_pre_mean(lb) for lb in np.arange(4,28)]\n",
    "post_means = [compute_post_mean(lb) for lb in np.arange(4,28)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(pre_means, label=\"during-pre\")\n",
    "ax.plot(post_means, label=\"during-post\")\n",
    "ax.axhline(y=1, label='unity', color='red', linestyle='--');\n",
    "ax.set_title(\"Pre- and Post- ratios versus lookback/lookforward time\")\n",
    "ax.set_ylabel(\"<Var(during) / Var(Pre/Post)>\")\n",
    "ax.set_xlabel(\"Lookback/forward time\")\n",
    "ax.grid(True)\n",
    "ax.set_ylim(0,np.nanmax(post_means))\n",
    "plt.legend();\n",
    "plt.savefig(\"../figures/mean_ratio_vs_lookbackforward.png\")\n",
    "plt.savefig('../literature/our_work/mean_ratio_vs_lookbackforward.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ae8742",
   "metadata": {},
   "source": [
    "### P-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9704b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of pre-ratios that are below unity\n",
    "n_below_pre = len([rat for rat in rats_pre if rat < 1])\n",
    "\n",
    "# Compute number of post-ratios that are below unity\n",
    "n_below_post = len([rat for rat in rats_post if rat < 1])\n",
    "\n",
    "# Number of total events\n",
    "n_events = len(rats_pre)\n",
    "\n",
    "# Compute p-score, assuming the null-hypothesis is unity\n",
    "pre_p_score = scipy.stats.binomtest(n_below_pre, n_events, p=0.5).pvalue\n",
    "post_p_score = scipy.stats.binomtest(n_below_post, n_events, p=0.5).pvalue\n",
    "\n",
    "print(f\"\"\"\n",
    "    Number of events with lower Var during (pre): {n_below_pre}\n",
    "    Number of events with lower Var during (post): {n_below_post}\n",
    "    Events with lower Var during (pre) - p score: {pre_p_score}\n",
    "    Events with lower Var during (post) - p score: {post_p_score}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac0fbd4",
   "metadata": {},
   "source": [
    "## Study B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053dea95",
   "metadata": {},
   "source": [
    "### Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a2ec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tranquility = sp_file['Tranquility']\n",
    "use_roll_avg = False\n",
    "\n",
    "if use_roll_avg:\n",
    "    avg_x_days = 7\n",
    "    print(f\"{FancyColors.IMPORTANT}Using {avg_x_days}-day rolling average of Tranquility{FancyColors.RESET}\")\n",
    "    tranquility = tranquility.rolling(window=avg_x_days).mean()\n",
    "\n",
    "\n",
    "sp_file_time = sp_file['jd'].values\n",
    "ps_file_time = file['Earth']['jd'][:]\n",
    "\n",
    "# Use entire aligned date range\n",
    "time_lower = np.max([sp_file_time[0], ps_file_time[0]]) #Time('2015-06-05', format='iso').jd\n",
    "time_higher = np.min([sp_file_time[-1], ps_file_time[-1]]) # Time('2019-12-05', format='iso').jd\n",
    "\n",
    "# Generate the time mask\n",
    "MASK = ( timearray >= time_lower ) & ( timearray <= time_higher)\n",
    "SP_MASK = ( sp_file['jd'] >= time_lower ) & ( sp_file['jd'] <= time_higher )\n",
    "\n",
    "# Slice the time array\n",
    "timearray_sliced = timearray[ MASK ]\n",
    "sp_time_sliced = sp_file_time[ SP_MASK ]\n",
    "\n",
    "# Ensure time arrays are aligned\n",
    "print(f\"Time arrays are aligned: {np.all(timearray_sliced == sp_time_sliced)}\")\n",
    "\n",
    "# Slice the angle array\n",
    "angles_sliced = angles[ MASK ]\n",
    "\n",
    "# Create a boolean array to show where 90 degree events occur\n",
    "events_bool = ( angles_sliced >= __ANGLE_LIMIT_LOWER__ ) & ( angles_sliced <= __ANGLE_LIMIT_HIGHER__)\n",
    "\n",
    "# Slice the tranquility array to the times as well\n",
    "tranq_masked = tranquility[SP_MASK].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8a6d5d",
   "metadata": {},
   "source": [
    "### Compute post X-day difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03161f5b",
   "metadata": {},
   "source": [
    "First use the local-averaging approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988dc7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 weeks after and 10 day local-averaging times span gives most positive differences, 22/26 -> pscore 0.0005335211753845215\n",
    "# 5 weeks after and 4 day local-averaging times gives most negative differences, 15/26 -> pscore 0.557197093963623\n",
    "\n",
    "weeks_after = 3\n",
    "endof_points = 10 # Number of points to use for local averaging around end of event\n",
    "after_points = 10 # Number of points to use for local averaging around x-weeks after\n",
    "\n",
    "delta_info = angle_window_post_event_difference(\n",
    "    timearray_sliced, \n",
    "    angles_sliced, \n",
    "    tranq_masked, \n",
    "    __ANGLE_LIMIT_LOWER__,\n",
    "    __ANGLE_LIMIT_HIGHER__,\n",
    "    weeks_after, # weeks after\n",
    "    n_points_at_end = endof_points,\n",
    "    n_points_after = after_points\n",
    ")\n",
    "print(f\"Number of windows: {len(delta_info)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df97d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = [event['difference'] for event in delta_info]\n",
    "errors = [event['error'] for event in delta_info]\n",
    "event_times_jd = [event['end_time'] for event in delta_info]\n",
    "\n",
    "# Convert Julian dates to datetime\n",
    "event_dates = Time(event_times_jd, format='jd').to_datetime()\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Plot with error bars\n",
    "ax.errorbar(event_dates, differences, yerr=errors,\n",
    "            fmt='o', markersize=8, capsize=5, capthick=2,\n",
    "            color='steelblue', ecolor='lightsteelblue', elinewidth=2,\n",
    "            label='Difference ± Error')\n",
    "\n",
    "# Add horizontal line at zero for reference\n",
    "ax.axhline(y=0, color='red', linestyle='--', linewidth=1.5, alpha=0.7, label='Zero line')\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel('Event End Date', fontsize=12)\n",
    "ax.set_ylabel('Difference (After - At End)', fontsize=12)\n",
    "ax.set_title('Post-Event Difference in Data with Error Bars', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Format x-axis to show dates nicely\n",
    "import matplotlib.dates as mdates\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "\n",
    "plt.savefig(\"../figures/3weekdeltas.png\")\n",
    "plt.savefig('../literature/our_work/3weekdeltas.png')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c299e35",
   "metadata": {},
   "source": [
    "Compute the number of events where the difference is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f28cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_diff_decrease = len([info['difference'] for info in delta_info if info['difference']<0])\n",
    "print(f\"Number of events where the {weeks_after}-weeks after is less than the end-of-event: {FancyColors.YELLOW}{n_diff_decrease}{FancyColors.RESET}\")\n",
    "print(f\"Associated binomial p-score: {binom_p(n_diff_decrease, len(delta_info))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d328394",
   "metadata": {},
   "source": [
    "Now use the gradient approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bac50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks_after = 3\n",
    "\n",
    "delta_grad_info = angle_window_post_event_gradient(\n",
    "    timearray_sliced, \n",
    "    angles_sliced, \n",
    "    tranq_masked, \n",
    "    __ANGLE_LIMIT_LOWER__,\n",
    "    __ANGLE_LIMIT_HIGHER__,\n",
    "    weeks_after, # weeks after\n",
    ")\n",
    "print(f\"Number of windows: {len(delta_grad_info)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb61881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = [event['gradient'] for event in delta_grad_info]\n",
    "errors = [event['gradient_error'] for event in delta_grad_info]\n",
    "event_times_jd = [event['end_time'] for event in delta_grad_info]\n",
    "\n",
    "# Convert Julian dates to datetime\n",
    "event_dates = Time(event_times_jd, format='jd').to_datetime()\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Plot with error bars\n",
    "ax.errorbar(event_dates, differences, yerr=errors,\n",
    "            fmt='o', markersize=8, capsize=5, capthick=2,\n",
    "            color='steelblue', ecolor='lightsteelblue', elinewidth=2,\n",
    "            label='Gradient ± Error')\n",
    "\n",
    "# Add horizontal line at zero for reference\n",
    "ax.axhline(y=0, color='red', linestyle='--', linewidth=1.5, alpha=0.7, label='Zero line')\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel('Event End Date', fontsize=12)\n",
    "ax.set_ylabel('Gradient (After - At End)', fontsize=12)\n",
    "ax.set_title('Post-Event Gradient in Data with Error Bars', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Format x-axis to show dates nicely\n",
    "import matplotlib.dates as mdates\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b7b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grad_decrease = len([info['gradient'] for info in delta_grad_info if info['gradient']<0])\n",
    "print(f\"Number of events where the {weeks_after}-weeks after is less than the end-of-event: {FancyColors.YELLOW}{n_grad_decrease}{FancyColors.RESET}\")\n",
    "print(f\"Associated binomial p-score: {binom_p(n_grad_decrease, len(delta_info))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5c22d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_p_score = scipy.stats.binomtest(n_diff_decrease, len(delta_grad_info), p=0.5).pvalue\n",
    "grad_p_score = scipy.stats.binomtest(n_grad_decrease, len(delta_grad_info), p=0.5).pvalue\n",
    "diff_p_score_above = scipy.stats.binomtest(len(delta_grad_info) - n_diff_decrease, len(delta_grad_info), p=0.5).pvalue\n",
    "grad_p_score_above = scipy.stats.binomtest(len(delta_grad_info) - n_grad_decrease, len(delta_grad_info), p=0.5).pvalue\n",
    "\n",
    "print('='*10, ' for n points below ', '='*10)\n",
    "print(f\"Difference-based p-score (below) with {endof_points} local points for end-mean and {after_points} local points for after-mean: {diff_p_score}\")\n",
    "print(f\"Gradient-based p-score: {grad_p_score}\")\n",
    "print('\\n', '='*10, ' for n points above ', '='*10)\n",
    "print(f\"Difference-based p-score (above) with {endof_points} local points for end-mean and {after_points} local points for after-mean: {diff_p_score_above}\")\n",
    "print(f\"Gradient-based p-score (above) : {grad_p_score_above}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca9427e",
   "metadata": {},
   "source": [
    "<span style=\"color: cyan;\">\n",
    "Now we analyze the change in the delta (using the local-averaged difference) versus lookforward time\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d2f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookforward = np.linspace(0,10,11, dtype=np.int64)\n",
    "local_avg = np.linspace(1,10, 10, dtype=np.int64)\n",
    "total_data = np.zeros((len(local_avg), len(lookforward), len(delta_info)))\n",
    "\n",
    "for inx, loc_avg in enumerate(local_avg):\n",
    "    mean_diffs = []\n",
    "    for wk_after in np.linspace(0,10,11,dtype=np.int64):\n",
    "        delta_info = angle_window_post_event_difference(\n",
    "            timearray_sliced, \n",
    "            angles_sliced, \n",
    "            tranq_masked, \n",
    "            __ANGLE_LIMIT_LOWER__,\n",
    "            __ANGLE_LIMIT_HIGHER__,\n",
    "            wk_after, # weeks after\n",
    "            n_points_at_end = loc_avg,\n",
    "            n_points_after = loc_avg\n",
    "        )\n",
    "        diffs = [info['difference'] for info in delta_info]\n",
    "        mean_diffs.append(diffs)\n",
    "    total_data[inx] = mean_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06623186",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = lookforward  # Your time array\n",
    "\n",
    "# Create figure with vertically stacked subplots\n",
    "n_plots = len(total_data)\n",
    "fig, axes = plt.subplots(n_plots, 1, figsize=(14, 3*n_plots), sharex=True)\n",
    "\n",
    "# If only one plot, axes won't be an array, so make it one\n",
    "if n_plots == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Plot each data array\n",
    "for i, (ax, data) in enumerate(zip(axes, total_data)):\n",
    "    # Convert time to datetime for nice formatting\n",
    "    mdata = np.mean(data, axis=1)\n",
    "    ax.plot(x_data, mdata, linewidth=1.5)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(np.min(mdata), np.max(mdata))\n",
    "\n",
    "    ax.set_ylabel(f\"Local avg: {local_avg[i]}\")\n",
    "\n",
    "    # Only show x-label on bottom plot\n",
    "    if i == n_plots - 1:\n",
    "        ax.set_xlabel('Lookforward time', fontsize=12)\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle('Stacked Time Series Data', fontsize=14, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=0)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1503f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdat = []\n",
    "for inx, loc_avg in enumerate(local_avg):\n",
    "    for linx, lkfor in enumerate(lookforward):\n",
    "        dat = total_data[inx, linx]\n",
    "        n_below = len([d for d in dat if d<0])\n",
    "        total_points = len(dat)\n",
    "        #print(f\"Local avg points: {loc_avg}  look forward: {lkfor}   n_below: {n_below}/{total_points}\")\n",
    "        tdat.append( (loc_avg, lkfor, n_below) )\n",
    "[print(tup) for tup in tdat if tup[2] >= 15]\n",
    "print('\\n')\n",
    "print([tup for tup in tdat if 0<tup[2] <=5])\n",
    "print(f\" p-score: {binom_p(26-4, 26)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ebdd02",
   "metadata": {},
   "source": [
    "<span style=\"color: cyan;\">\n",
    "We see that to maximize the differences, we pick a 3-week lookforward time and a 10 day local averaging span.\n",
    "\n",
    "This produces a pscore of 0.00053 for 4 / 26 days with negative difference\n",
    "\n",
    "To show randomness in the tranquility data, we repeat this calculation for randomly selected 3-week spans\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1216ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = random_span_difference_null_distribution(\n",
    "    tranq_masked,\n",
    "    span_weeks = 3,\n",
    "    n_local_points = 10,\n",
    "    n_trials =5000\n",
    ")\n",
    "\n",
    "print(f\"\"\"\n",
    "Percent number of positive differences: {results[\"percent_positive\"]}\n",
    "Percent number of negative differences: {results[\"percent_negative\"]}\n",
    "Percent number of zero differences: {results[\"percent_zero\"]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4dd761",
   "metadata": {},
   "source": [
    "# Study C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e1998d",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8be441d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time arrays are aligned: True\n"
     ]
    }
   ],
   "source": [
    "tranquility = sp_file['Tranquility']\n",
    "use_roll_avg = False\n",
    "\n",
    "if use_roll_avg:\n",
    "    avg_x_days = 7\n",
    "    print(f\"{FancyColors.IMPORTANT}Using {avg_x_days}-day rolling average of Tranquility{FancyColors.RESET}\")\n",
    "    tranquility = tranquility.rolling(window=avg_x_days).mean()\n",
    "\n",
    "\n",
    "sp_file_time = sp_file['jd'].values\n",
    "ps_file_time = file['Earth']['jd'][:]\n",
    "\n",
    "# Use entire aligned date range\n",
    "time_lower = np.max([sp_file_time[0], ps_file_time[0]]) #Time('2015-06-05', format='iso').jd\n",
    "time_higher = np.min([sp_file_time[-1], ps_file_time[-1]]) # Time('2019-12-05', format='iso').jd\n",
    "\n",
    "# Generate the time mask\n",
    "MASK = ( timearray >= time_lower ) & ( timearray <= time_higher)\n",
    "SP_MASK = ( sp_file['jd'] >= time_lower ) & ( sp_file['jd'] <= time_higher )\n",
    "\n",
    "# Slice the time array\n",
    "timearray_sliced = timearray[ MASK ]\n",
    "sp_time_sliced = sp_file_time[ SP_MASK ]\n",
    "\n",
    "# Ensure time arrays are aligned\n",
    "print(f\"Time arrays are aligned: {np.all(timearray_sliced == sp_time_sliced)}\")\n",
    "\n",
    "# Slice the angle array\n",
    "angles_sliced = angles[ MASK ]\n",
    "\n",
    "# Create a boolean array to show where 90 degree events occur\n",
    "events_bool = ( angles_sliced >= __ANGLE_LIMIT_LOWER__ ) & ( angles_sliced <= __ANGLE_LIMIT_HIGHER__)\n",
    "\n",
    "# Slice the tranquility array to the times as well\n",
    "tranq_masked = tranquility[SP_MASK].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be587e45",
   "metadata": {},
   "source": [
    "## Recompute tranquility decrease post-events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7da8de83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of windows: 26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['end_idx', 'after_idx', 'end_time', 'mean_at_end', 'mean_after', 'difference', 'std_at_end', 'std_after', 'error', 'data_at_end', 'data_after'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 weeks after and 10 day local-averaging times span gives most positive differences, 22/26 -> pscore 0.0005335211753845215\n",
    "# 5 weeks after and 4 day local-averaging times gives most negative differences, 15/26 -> pscore 0.557197093963623\n",
    "\n",
    "weeks_after = 3\n",
    "endof_points = 10 # Number of points to use for local averaging around end of event\n",
    "after_points = 10 # Number of points to use for local averaging around x-weeks after\n",
    "\n",
    "delta_info = angle_window_post_event_difference(\n",
    "    timearray_sliced, \n",
    "    angles_sliced, \n",
    "    tranq_masked, \n",
    "    __ANGLE_LIMIT_LOWER__,\n",
    "    __ANGLE_LIMIT_HIGHER__,\n",
    "    weeks_after, # weeks after\n",
    "    n_points_at_end = endof_points,\n",
    "    n_points_after = after_points\n",
    ")\n",
    "print(f\"Number of windows: {len(delta_info)}\")\n",
    "delta_info[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e0de4",
   "metadata": {},
   "source": [
    "## Normalize planetary center vectors magnitudes to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dfa95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mags = np.linalg.norm(g2_barycenter, axis=1)\n",
    "min_mag = min(mags)\n",
    "max_mag = max(mags)\n",
    "mags_normed = (mags - min_mag) / (max_mag- min_mag)\n",
    "g2_center_normed = g2_barycenter * (mags_normed / mags)[:,None]\n",
    "\n",
    "# Slice the normed centers to be aligned with the sunspot data\n",
    "g2_center_normed = g2_center_normed[ MASK ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a97b1e",
   "metadata": {},
   "source": [
    "## Compute the planetary center magnitude at the events and compare to the tranquility deltas for each event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdeba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pairs = []\n",
    "for info in delta_info:\n",
    "    delta = info['difference']\n",
    "    center_mag_at_event_start = np.linalg.norm(g2_center_normed[info['end_idx']])\n",
    "    center_mag_at_after_event = np.linalg.norm(g2_center_normed[info['after_idx']])\n",
    "    mean_mag = np.mean([center_mag_at_after_event, center_mag_at_event_start])\n",
    "    data_pairs.append(\n",
    "        (delta, mean_mag)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a5af2b",
   "metadata": {},
   "source": [
    "## Plot the magnitude of the second planetary group center versus the difference in solar tranquility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595f391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = np.array([d[0] for d in data_pairs])\n",
    "mags = np.array([d[1] for d in data_pairs])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.plot(mags, deltas, 'o')\n",
    "ax.set_xlabel(r'$||g2_{center}||$')\n",
    "ax.set_ylabel(\"Tranquility Deltas\")\n",
    "ax.set_title(\"Magnitude of Tranquility difference 3-weeks post event versus magnitude of G2 center\")\n",
    "\n",
    "\n",
    "\n",
    "plt.savefig(\"../figures/3weekdeltas_g2pos.png\")\n",
    "plt.savefig('../literature/our_work/3weekdeltas_g2pos.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c1832",
   "metadata": {},
   "source": [
    "## Compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cf17fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation coefficient\n",
    "pear_corr, pscore = scipy.stats.pearsonr(mags, deltas)\n",
    "n_sigma = scipy.stats.norm.ppf(1 - pscore/2)\n",
    "print(f\"Pearson correlation: {pear_corr}\")\n",
    "print(f\"p-score: {pscore}\")\n",
    "print(f\"Number of sigma: {n_sigma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4a52c8",
   "metadata": {},
   "source": [
    "The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Pearson correlation at least as extreme as the one computed from these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d09020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13.1 (pyenv)",
   "language": "python",
   "name": "pyenv-3.13.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
